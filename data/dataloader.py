"""
æ•°æ®åŠ è½½å’Œå¤„ç†æ¨¡å—
è´Ÿè´£å¤„ç† NOAA Global Surface Summary of Day æ•°æ®é›†
è¾“å‡ºæ¸…æ´—åçš„ CSV æ–‡ä»¶ä¾› HMM å’Œ Baseline æ¨¡å—ä½¿ç”¨
"""

import os
import gzip
import tarfile
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class GSODDataLoader:
    """NOAA GSOD æ•°æ®åŠ è½½å’Œå¤„ç†å™¨"""
    
    def __init__(self, data_root=None, output_dir=None, n_bins=5, discretize=True):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨
        
        Args:
            data_root: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            n_bins: ç¦»æ•£åŒ–çš„ç»„æ•°ï¼ˆé»˜è®¤5ç»„ï¼Œæ‰€æœ‰è¿ç»­ç‰¹å¾ä½¿ç”¨ç›¸åŒç»„æ•°ï¼‰
            discretize: æ˜¯å¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ï¼ˆé»˜è®¤Trueï¼‰
        """
        if data_root is None:
            # è‡ªåŠ¨æ£€æµ‹æ•°æ®è·¯å¾„
            current_file = Path(__file__).resolve()
            proj_root = current_file.parent.parent.parent
            data_root = proj_root / "kaggle_data" / "datasets" / "noaa" / \
                       "noaa-global-surface-summary-of-the-day" / "versions" / "2"
        
        self.data_root = Path(data_root)
        self.gsod_dir = self.data_root / "gsod_all_years"
        self.station_info_path = self.data_root / "isd-history.csv"
        
        if output_dir is None:
            output_dir = Path(__file__).parent / "processed"
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)
        
        # ç¦»æ•£åŒ–å‚æ•°
        self.n_bins = n_bins
        self.discretize = discretize
        
        # æ•°æ®æ ¼å¼å®šä¹‰ï¼ˆåŸºäº GSOD æ–‡æ¡£ï¼‰
        self.missing_values = {
            'TEMP': 9999.9,
            'DEWP': 9999.9,
            'SLP': 9999.9,
            'STP': 9999.9,
            'VISIB': 999.9,
            'WDSP': 999.9,
            'MXSPD': 999.9,
            'GUST': 999.9,
            'MAX': 9999.9,
            'MIN': 9999.9,
            'PRCP': 99.99,
            'SNDP': 999.9
        }
        
        # è¿ç»­ç‰¹å¾åˆ—è¡¨ï¼ˆéœ€è¦å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–çš„ç‰¹å¾ï¼‰
        self.continuous_features = [
            'mean_temp', 'dew_point', 'max_temp', 'min_temp',
            'sea_level_pressure', 'station_pressure',
            'visibility', 'wind_speed', 'max_wind_speed', 'wind_gust',
            'precipitation', 'snow_depth'
        ]
    
    def load_station_metadata(self):
        """
        åŠ è½½ç«™ç‚¹å…ƒæ•°æ®
        
        Returns:
            DataFrame: åŒ…å«ç«™ç‚¹ä¿¡æ¯çš„æ•°æ®æ¡†
        """
        print("ğŸ“‹ åŠ è½½ç«™ç‚¹å…ƒæ•°æ®...")
        df = pd.read_csv(self.station_info_path)
        
        # æ¸…ç†åˆ—å
        df.columns = df.columns.str.strip().str.replace('"', '')
        
        # åˆ›å»º site_id (USAF-WBAN)
        df['site_id'] = df['USAF'].astype(str).str.zfill(6) + '-' + \
                        df['WBAN'].astype(str).str.zfill(5)
        
        print(f"   âœ… åŠ è½½äº† {len(df)} ä¸ªç«™ç‚¹çš„å…ƒæ•°æ®")
        return df
    
    def parse_gsod_file(self, filepath):
        """
        è§£æå•ä¸ª GSOD æ–‡ä»¶ï¼ˆä½¿ç”¨å›ºå®šå®½åº¦æ ¼å¼ï¼‰
        
        Args:
            filepath: æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ .gz æˆ– .op æ–‡ä»¶ï¼‰
            
        Returns:
            DataFrame: è§£æåçš„æ•°æ®
        """
        try:
            # GSOD æ•°æ®æ ¼å¼å®šä¹‰ï¼ˆåŸºäºå®˜æ–¹æ–‡æ¡£ï¼‰
            colspecs = [
                (0, 6),     # STN
                (7, 12),    # WBAN  
                (14, 22),   # YEARMODA
                (24, 30),   # TEMP
                (31, 33),   # TEMP_COUNT
                (35, 41),   # DEWP
                (42, 44),   # DEWP_COUNT
                (46, 52),   # SLP
                (53, 55),   # SLP_COUNT
                (57, 63),   # STP
                (64, 66),   # STP_COUNT
                (68, 73),   # VISIB
                (74, 76),   # VISIB_COUNT
                (78, 83),   # WDSP
                (84, 86),   # WDSP_COUNT
                (88, 93),   # MXSPD
                (95, 100),  # GUST
                (102, 108), # MAX
                (108, 109), # MAX_FLAG
                (110, 116), # MIN
                (116, 117), # MIN_FLAG
                (118, 123), # PRCP
                (123, 124), # PRCP_FLAG
                (125, 130), # SNDP
                (132, 138)  # FRSHTT
            ]

            names = ['STN---', 'WBAN', 'YEARMODA', 'TEMP', 'TEMP_COUNT', 'DEWP', 'DEWP_COUNT',
                     'SLP', 'SLP_COUNT', 'STP', 'STP_COUNT', 'VISIB', 'VISIB_COUNT',
                     'WDSP', 'WDSP_COUNT', 'MXSPD', 'GUST', 'MAX', 'MAX_FLAG',
                     'MIN', 'MIN_FLAG', 'PRCP', 'PRCP_FLAG', 'SNDP', 'FRSHTT']
            
            # æ ¹æ®æ–‡ä»¶æ‰©å±•åé€‰æ‹©æ‰“å¼€æ–¹å¼
            if str(filepath).endswith('.gz'):
                with gzip.open(filepath, 'rt') as f:
                    df = pd.read_fwf(f, colspecs=colspecs, names=names, skiprows=1)
            else:
                df = pd.read_fwf(filepath, colspecs=colspecs, names=names, skiprows=1)
            
            return df
        except Exception as e:
            print(f"   âš ï¸  è§£ææ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return None
    
    def process_year_data(self, year, max_stations=None):
        """
        å¤„ç†æŒ‡å®šå¹´ä»½çš„æ•°æ®
        
        Args:
            year: å¹´ä»½ï¼ˆå¦‚ 2015ï¼‰
            max_stations: æœ€å¤§å¤„ç†ç«™ç‚¹æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼ŒNoneè¡¨ç¤ºå¤„ç†å…¨éƒ¨ï¼‰
            
        Returns:
            DataFrame: å¤„ç†åçš„æ•°æ®
        """
        print(f"\nğŸ“… å¤„ç† {year} å¹´æ•°æ®...")
        
        tar_path = self.gsod_dir / f"gsod_{year}.tar"
        if not tar_path.exists():
            print(f"   âŒ æ–‡ä»¶ä¸å­˜åœ¨: {tar_path}")
            return None
        
        all_data = []
        
        # æå–å¹¶å¤„ç† tar æ–‡ä»¶
        with tarfile.open(tar_path, 'r') as tar:
            members = tar.getmembers()
            
            # é™åˆ¶å¤„ç†çš„ç«™ç‚¹æ•°é‡ï¼ˆç”¨äºæµ‹è¯•ï¼‰
            if max_stations:
                members = members[:max_stations]
            
            print(f"   å¤„ç† {len(members)} ä¸ªç«™ç‚¹æ–‡ä»¶...")
            
            for member in tqdm(members, desc=f"   è§£æ {year}"):
                if not member.name.endswith('.op.gz') and not member.name.endswith('.op'):
                    continue
                
                # æå–æ–‡ä»¶
                f = tar.extractfile(member)
                if f is None:
                    continue
                
                # è¯»å–å†…å®¹
                try:
                    # GSOD æ•°æ®æ ¼å¼å®šä¹‰ï¼ˆåŸºäºå®˜æ–¹æ–‡æ¡£ï¼‰
                    colspecs = [
                        (0, 6),     # STN
                        (7, 12),    # WBAN  
                        (14, 22),   # YEARMODA
                        (24, 30),   # TEMP
                        (31, 33),   # TEMP_COUNT
                        (35, 41),   # DEWP
                        (42, 44),   # DEWP_COUNT
                        (46, 52),   # SLP
                        (53, 55),   # SLP_COUNT
                        (57, 63),   # STP
                        (64, 66),   # STP_COUNT
                        (68, 73),   # VISIB
                        (74, 76),   # VISIB_COUNT
                        (78, 83),   # WDSP
                        (84, 86),   # WDSP_COUNT
                        (88, 93),   # MXSPD
                        (95, 100),  # GUST
                        (102, 108), # MAX
                        (108, 109), # MAX_FLAG
                        (110, 116), # MIN
                        (116, 117), # MIN_FLAG
                        (118, 123), # PRCP
                        (123, 124), # PRCP_FLAG
                        (125, 130), # SNDP
                        (132, 138)  # FRSHTT
                    ]

                    names = ['STN---', 'WBAN', 'YEARMODA', 'TEMP', 'TEMP_COUNT', 'DEWP', 'DEWP_COUNT',
                             'SLP', 'SLP_COUNT', 'STP', 'STP_COUNT', 'VISIB', 'VISIB_COUNT',
                             'WDSP', 'WDSP_COUNT', 'MXSPD', 'GUST', 'MAX', 'MAX_FLAG',
                             'MIN', 'MIN_FLAG', 'PRCP', 'PRCP_FLAG', 'SNDP', 'FRSHTT']
                    
                    if member.name.endswith('.gz'):
                        content = gzip.decompress(f.read()).decode('utf-8')
                    else:
                        content = f.read().decode('utf-8')
                    
                    # è§£æä¸º DataFrame (ä½¿ç”¨å›ºå®šå®½åº¦æ ¼å¼)
                    from io import StringIO
                    df = pd.read_fwf(StringIO(content), colspecs=colspecs, names=names, skiprows=1)
                    
                    if len(df) > 0:
                        all_data.append(df)
                
                except Exception as e:
                    continue
        
        if not all_data:
            print(f"   âŒ æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ•°æ®")
            return None
        
        # åˆå¹¶æ‰€æœ‰æ•°æ®
        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"   âœ… æˆåŠŸå¤„ç† {len(combined_df)} è¡Œæ•°æ®ï¼Œæ¥è‡ª {len(all_data)} ä¸ªç«™ç‚¹")
        
        return combined_df
    
    def clean_and_transform(self, df):
        """
        æ¸…æ´—å’Œè½¬æ¢æ•°æ®
        
        Args:
            df: åŸå§‹æ•°æ®æ¡†
            
        Returns:
            DataFrame: æ¸…æ´—åçš„æ•°æ®æ¡†
        """
        print("\nğŸ§¹ æ¸…æ´—å’Œè½¬æ¢æ•°æ®...")
        
        # åˆ›å»º site_id
        df['site_id'] = df['STN---'].astype(str).str.zfill(6) + '-' + \
                        df['WBAN'].astype(str).str.zfill(5)
        
        # è½¬æ¢æ—¥æœŸæ ¼å¼ï¼ˆå¤„ç†å¯èƒ½çš„å¼‚å¸¸å€¼ï¼‰
        df['YEARMODA'] = df['YEARMODA'].astype(str).str.zfill(8)
        df['date'] = pd.to_datetime(df['YEARMODA'], format='%Y%m%d', errors='coerce')
        
        # åˆ é™¤æ—¥æœŸæ— æ•ˆçš„è¡Œ
        invalid_dates = df['date'].isna().sum()
        if invalid_dates > 0:
            print(f"   âš ï¸  åˆ é™¤ {invalid_dates} è¡Œæ— æ•ˆæ—¥æœŸæ•°æ®")
            df = df.dropna(subset=['date'])
        
        # å¤„ç†ç¼ºå¤±å€¼
        for col, missing_val in self.missing_values.items():
            if col in df.columns:
                df[col] = df[col].replace(missing_val, np.nan)
        
        # è§£æ FRSHTT æ ‡è®°ï¼ˆå¤©æ°”æ¡ä»¶ç¼–ç ï¼‰
        # FRSHTT æ˜¯ä¸€ä¸ª6ä½æ•°å­—ï¼Œæ¯ä½ä»£è¡¨ä¸€ç§å¤©æ°”ç°è±¡çš„æœ‰æ— 
        if 'FRSHTT' in df.columns:
            frshtt_str = df['FRSHTT'].astype(str).str.zfill(6)
            df['fog'] = frshtt_str.str[0].astype(int)           # é›¾
            df['rain'] = frshtt_str.str[1].astype(int)          # é›¨/ç»†é›¨
            df['snow'] = frshtt_str.str[2].astype(int)          # é›ª/å†°ç²’
            df['hail'] = frshtt_str.str[3].astype(int)          # å†°é›¹
            df['thunder'] = frshtt_str.str[4].astype(int)       # é›·æš´
            df['tornado'] = frshtt_str.str[5].astype(int)       # é¾™å·é£
        
        # é€‰æ‹©å¹¶é‡å‘½åç‰¹å¾åˆ—
        feature_cols = {
            'site_id': 'site_id',
            'date': 'date',
            'TEMP': 'mean_temp',          # å¹³å‡æ¸©åº¦
            'DEWP': 'dew_point',          # éœ²ç‚¹
            'SLP': 'sea_level_pressure',  # æµ·å¹³é¢æ°”å‹
            'STP': 'station_pressure',    # ç«™ç‚¹æ°”å‹
            'VISIB': 'visibility',        # èƒ½è§åº¦
            'WDSP': 'wind_speed',         # å¹³å‡é£é€Ÿ
            'MXSPD': 'max_wind_speed',    # æœ€å¤§é£é€Ÿ
            'GUST': 'wind_gust',          # é˜µé£
            'MAX': 'max_temp',            # æœ€é«˜æ¸©åº¦
            'MIN': 'min_temp',            # æœ€ä½æ¸©åº¦
            'PRCP': 'precipitation',      # é™æ°´é‡
            'SNDP': 'snow_depth',         # é›ªæ·±
            'fog': 'fog',
            'rain': 'rain',
            'snow': 'snow',
            'hail': 'hail',
            'thunder': 'thunder',
            'tornado': 'tornado'
        }
        
        # åªä¿ç•™å­˜åœ¨çš„åˆ—
        available_cols = {k: v for k, v in feature_cols.items() if k in df.columns}
        df_cleaned = df[list(available_cols.keys())].rename(columns=available_cols)
        
        # æŒ‰ site_id å’Œ date æ’åºï¼ˆé‡è¦ï¼ç¬¦åˆREADMEè¦æ±‚ï¼‰
        df_cleaned = df_cleaned.sort_values(['site_id', 'date']).reset_index(drop=True)
        
        print(f"   âœ… æ¸…æ´—å®Œæˆï¼Œä¿ç•™ {len(df_cleaned)} è¡Œï¼Œ{len(df_cleaned.columns)} åˆ—ç‰¹å¾")
        
        # å¦‚æœå¯ç”¨ç¦»æ•£åŒ–ï¼Œè¿›è¡Œå½’ä¸€åŒ–å’Œåˆ†ç»„
        if self.discretize:
            df_cleaned = self.normalize_and_discretize(df_cleaned)
        
        return df_cleaned
    
    def normalize_and_discretize(self, df):
        """
        å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–
        
        Args:
            df: æ¸…æ´—åçš„æ•°æ®æ¡†
            
        Returns:
            DataFrame: å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–åçš„æ•°æ®æ¡†
        """
        print(f"\nğŸ”¢ å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–è¿ç»­ç‰¹å¾...")
        print(f"   ç¦»æ•£åŒ–ç»„æ•°: {self.n_bins} ç»„")
        
        df_processed = df.copy()
        
        # å­˜å‚¨å½’ä¸€åŒ–å‚æ•°ï¼ˆç”¨äºæ–‡æ¡£è®°å½•ï¼‰
        normalization_info = {}
        
        for feature in self.continuous_features:
            if feature not in df_processed.columns:
                continue
            
            # è·å–éç¼ºå¤±æ•°æ®
            valid_mask = df_processed[feature].notna()
            valid_data = df_processed.loc[valid_mask, feature]
            
            if len(valid_data) == 0:
                print(f"   âš ï¸  {feature}: å…¨éƒ¨ç¼ºå¤±ï¼Œè·³è¿‡")
                continue
            
            # 1. å½’ä¸€åŒ–åˆ° [0, 1]
            min_val = valid_data.min()
            max_val = valid_data.max()
            
            if max_val > min_val:
                normalized = (valid_data - min_val) / (max_val - min_val)
                
                # 2. ç¦»æ•£åŒ–ä¸º n_bins ç»„ï¼ˆä½¿ç”¨ç­‰å®½åˆ†ç®±ï¼‰
                # ç»„æ ‡ç­¾: 0, 1, 2, ..., n_bins-1
                discretized = pd.cut(normalized, 
                                    bins=self.n_bins, 
                                    labels=range(self.n_bins),
                                    include_lowest=True)
                
                # è½¬æ¢ä¸ºæ•´æ•°
                discretized = discretized.astype(float)  # å…ˆè½¬floatä»¥å¤„ç†NaN
                
                # ä¿å­˜åŸå§‹åˆ—ï¼ˆä½œä¸º feature_rawï¼‰
                df_processed[f'{feature}_raw'] = df_processed[feature]
                
                # æ›´æ–°ä¸»åˆ—ä¸ºç¦»æ•£åŒ–åçš„å€¼
                df_processed.loc[valid_mask, feature] = discretized
                
                # è®°å½•å½’ä¸€åŒ–ä¿¡æ¯
                normalization_info[feature] = {
                    'min': min_val,
                    'max': max_val,
                    'bins': self.n_bins,
                    'unique_values': len(valid_data.unique())
                }
                
                print(f"   âœ… {feature:25s}: [{min_val:8.2f}, {max_val:8.2f}] â†’ [{0}, {self.n_bins-1}]")
            else:
                print(f"   âš ï¸  {feature}: å€¼èŒƒå›´ä¸º0ï¼Œè·³è¿‡ç¦»æ•£åŒ–")
        
        # ä¿å­˜å½’ä¸€åŒ–ä¿¡æ¯åˆ°æ–‡ä»¶
        info_path = self.output_dir / "normalization_info.txt"
        with open(info_path, 'w') as f:
            f.write("è¿ç»­ç‰¹å¾å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ä¿¡æ¯\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ç¦»æ•£åŒ–ç»„æ•°: {self.n_bins}\n")
            f.write(f"ç¦»æ•£åŒ–æ–¹æ³•: ç­‰å®½åˆ†ç®± (equal-width binning)\n")
            f.write(f"ç»„æ ‡ç­¾: 0, 1, 2, ..., {self.n_bins-1}\n\n")
            
            for feature, info in normalization_info.items():
                f.write(f"\n{feature}:\n")
                f.write(f"  åŸå§‹èŒƒå›´: [{info['min']:.2f}, {info['max']:.2f}]\n")
                f.write(f"  å”¯ä¸€å€¼æ•°: {info['unique_values']}\n")
                f.write(f"  ç¦»æ•£åŒ–å: {info['bins']} ç»„ (0-{info['bins']-1})\n")
        
        print(f"\n   ğŸ“„ å½’ä¸€åŒ–ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
        print(f"   âœ… ç¦»æ•£åŒ–å®Œæˆ: {len(normalization_info)} ä¸ªè¿ç»­ç‰¹å¾")
        
        return df_processed
    
    def save_processed_data(self, df, filename='processed_weather_data.csv'):
        """
        ä¿å­˜å¤„ç†åçš„æ•°æ®
        
        Args:
            df: å¤„ç†åçš„æ•°æ®æ¡†
            filename: è¾“å‡ºæ–‡ä»¶å
        """
        output_path = self.output_dir / filename
        df.to_csv(output_path, index=False)
        print(f"\nğŸ’¾ æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        print(f"   - è¡Œæ•°: {len(df)}")
        print(f"   - åˆ—æ•°: {len(df.columns)}")
        print(f"   - ç«™ç‚¹æ•°: {df['site_id'].nunique()}")
        print(f"   - æ—¥æœŸèŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
        
        # å¦‚æœå¯ç”¨äº†ç¦»æ•£åŒ–ï¼Œæ˜¾ç¤ºç›¸å…³ä¿¡æ¯
        if self.discretize:
            raw_cols = [c for c in df.columns if c.endswith('_raw')]
            print(f"   - ç¦»æ•£åŒ–ç‰¹å¾æ•°: {len(self.continuous_features)}")
            print(f"   - ç¦»æ•£åŒ–ç»„æ•°: {self.n_bins}")
            print(f"   - åŸå§‹å€¼åˆ—ï¼ˆ*_rawï¼‰: {len(raw_cols)} ä¸ª")
    
    def process_multiple_years(self, years, max_stations_per_year=None):
        """
        å¤„ç†å¤šä¸ªå¹´ä»½çš„æ•°æ®
        
        Args:
            years: å¹´ä»½åˆ—è¡¨
            max_stations_per_year: æ¯å¹´æœ€å¤§å¤„ç†ç«™ç‚¹æ•°
            
        Returns:
            DataFrame: åˆå¹¶åçš„æ•°æ®
        """
        all_years_data = []
        
        for year in years:
            df = self.process_year_data(year, max_stations=max_stations_per_year)
            if df is not None:
                all_years_data.append(df)
        
        if not all_years_data:
            print("âŒ æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å¹´ä»½çš„æ•°æ®")
            return None
        
        # åˆå¹¶æ‰€æœ‰å¹´ä»½
        print(f"\nğŸ”— åˆå¹¶ {len(all_years_data)} ä¸ªå¹´ä»½çš„æ•°æ®...")
        combined_df = pd.concat(all_years_data, ignore_index=True)
        
        # æ¸…æ´—å’Œè½¬æ¢
        cleaned_df = self.clean_and_transform(combined_df)
        
        return cleaned_df
    
    def generate_summary_statistics(self, df):
        """
        ç”Ÿæˆæ•°æ®æ‘˜è¦ç»Ÿè®¡
        
        Args:
            df: å¤„ç†åçš„æ•°æ®æ¡†
        """
        print("\nğŸ“Š æ•°æ®æ‘˜è¦ç»Ÿè®¡:")
        print("=" * 60)
        
        print(f"\nåŸºæœ¬ä¿¡æ¯:")
        print(f"  - æ€»è¡Œæ•°: {len(df):,}")
        print(f"  - æ€»ç«™ç‚¹æ•°: {df['site_id'].nunique():,}")
        print(f"  - æ—¥æœŸèŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
        
        # æ¯ä¸ªç«™ç‚¹çš„å¹³å‡è§‚æµ‹å¤©æ•°
        obs_per_site = df.groupby('site_id').size()
        print(f"\næ¯ä¸ªç«™ç‚¹çš„è§‚æµ‹å¤©æ•°:")
        print(f"  - å¹³å‡: {obs_per_site.mean():.1f} å¤©")
        print(f"  - ä¸­ä½æ•°: {obs_per_site.median():.1f} å¤©")
        print(f"  - æœ€å°: {obs_per_site.min()} å¤©")
        print(f"  - æœ€å¤§: {obs_per_site.max()} å¤©")
        
        # ç‰¹å¾ç¼ºå¤±ç‡
        print(f"\nç‰¹å¾ç¼ºå¤±ç‡:")
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            missing_rate = df[col].isna().sum() / len(df) * 100
            print(f"  - {col}: {missing_rate:.2f}%")
        
        print("=" * 60)


def main():
    """ä¸»å‡½æ•°ï¼šå¤„ç†æ•°æ®é›†ä¸­æ‰€æœ‰å¹´ä»½çš„æ•°æ®"""
    
    print("ğŸŒ¤ï¸  NOAA GSOD æ•°æ®å¤„ç†å·¥å…·")
    print("=" * 60)
    
    # è¯¢é—®æ˜¯å¦ç¦»æ•£åŒ–
    print("\næ˜¯å¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ï¼Ÿ")
    print("1. æ˜¯ - å½’ä¸€åŒ–åˆ°[0,1]å¹¶ç¦»æ•£åŒ–ä¸ºNç»„ï¼ˆæ¨èç”¨äºHMMï¼‰")
    print("2. å¦ - ä¿æŒåŸå§‹è¿ç»­å€¼")
    
    discretize_choice = input("\nè¯·é€‰æ‹© (1/2) [é»˜è®¤: 1]: ").strip() or "1"
    discretize = (discretize_choice == "1")
    
    n_bins = 5  # é»˜è®¤5ç»„
    if discretize:
        n_bins_input = input(f"ç¦»æ•£åŒ–ç»„æ•° (3-10) [é»˜è®¤: 5]: ").strip()
        if n_bins_input.isdigit():
            n_bins = int(n_bins_input)
            n_bins = max(3, min(10, n_bins))  # é™åˆ¶åœ¨3-10ä¹‹é—´
        print(f"âœ… å°†ä½¿ç”¨ {n_bins} ç»„è¿›è¡Œç¦»æ•£åŒ–")
    else:
        print("âœ… å°†ä¿æŒè¿ç»­å€¼")
    
    # åˆ›å»ºåŠ è½½å™¨å®ä¾‹
    loader = GSODDataLoader(n_bins=n_bins, discretize=discretize)
    
    # åŠ è½½ç«™ç‚¹å…ƒæ•°æ®
    station_metadata = loader.load_station_metadata()
    
    # è‡ªåŠ¨æ£€æµ‹æ‰€æœ‰å¯ç”¨å¹´ä»½
    print("\nğŸ“… æ£€æµ‹å¯ç”¨å¹´ä»½...")
    available_years = []
    for tar_file in sorted(loader.gsod_dir.glob("gsod_*.tar")):
        year = int(tar_file.stem.split('_')[1])
        available_years.append(year)
    
    print(f"   æ‰¾åˆ° {len(available_years)} ä¸ªå¹´ä»½: {available_years[0]} - {available_years[-1]}")
    
    # å¤„ç†æ•°æ®é€‰é¡¹
    print("\né€‰é¡¹: å¤„ç†å“ªäº›å¹´ä»½çš„æ•°æ®ï¼Ÿ")
    print("1. å¿«é€Ÿæµ‹è¯•ï¼ˆ2015å¹´ï¼Œå‰50ä¸ªç«™ç‚¹ï¼‰")
    print("2. å•å¹´å®Œæ•´ï¼ˆ2015å¹´ï¼Œæ‰€æœ‰ç«™ç‚¹ï¼‰")
    print("3. è¿‘æœŸæ•°æ®ï¼ˆ1973-2019å¹´ï¼Œæ¨èï¼šæ•°æ®æ›´å®Œæ•´ï¼‰")
    print("4. å…¨éƒ¨æ•°æ®ï¼ˆ1901-2019å¹´ï¼Œæ‰€æœ‰ç«™ç‚¹ï¼‰")
    
    choice = input(f"\nè¯·é€‰æ‹© (1/2/3/4) [é»˜è®¤: 3]: ").strip() or "3"
    
    if choice == "1":
        # å¿«é€Ÿæµ‹è¯•ï¼š2015å¹´ï¼Œå‰50ä¸ªç«™ç‚¹
        print("\nğŸ§ª å¿«é€Ÿæµ‹è¯•æ¨¡å¼...")
        df = loader.process_year_data(2015, max_stations=50)
        if df is not None:
            cleaned_df = loader.clean_and_transform(df)
            loader.save_processed_data(cleaned_df, 'weather_data_2015_sample.csv')
            loader.generate_summary_statistics(cleaned_df)
    
    elif choice == "2":
        # å•å¹´å®Œæ•´
        print("\nğŸ“Š å¤„ç†å•å¹´å®Œæ•´æ•°æ®...")
        df = loader.process_year_data(2015)
        if df is not None:
            cleaned_df = loader.clean_and_transform(df)
            loader.save_processed_data(cleaned_df, 'weather_data_2015_full.csv')
            loader.generate_summary_statistics(cleaned_df)
    
    elif choice == "3":
        # è¿‘æœŸæ•°æ®ï¼ˆ1973å¹´åæ•°æ®æ›´å®Œæ•´ï¼‰
        print("\nğŸŒ å¤„ç†è¿‘æœŸæ•°æ®ï¼ˆ1973-2019ï¼‰...")
        print("   â° è¿™å¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´ï¼Œè¯·è€å¿ƒç­‰å¾…...")
        years = [y for y in available_years if y >= 1973]
        print(f"   å¤„ç† {len(years)} ä¸ªå¹´ä»½")
        cleaned_df = loader.process_multiple_years(years)
        if cleaned_df is not None:
            loader.save_processed_data(cleaned_df, 'weather_data_1973_2019_full.csv')
            loader.generate_summary_statistics(cleaned_df)
    
    elif choice == "4":
        # å…¨éƒ¨æ•°æ®
        print("\nğŸŒ å¤„ç†å…¨éƒ¨å†å²æ•°æ®ï¼ˆ1901-2019ï¼‰...")
        print("   âš ï¸  è­¦å‘Šï¼šè¿™å°†å¤„ç†è¶…è¿‡100å¹´çš„æ•°æ®ï¼Œå¯èƒ½éœ€è¦æ•°å°æ—¶ï¼")
        confirm = input("   ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no) [no]: ").strip().lower()
        
        if confirm == "yes":
            print(f"   å¤„ç† {len(available_years)} ä¸ªå¹´ä»½")
            cleaned_df = loader.process_multiple_years(available_years)
            if cleaned_df is not None:
                loader.save_processed_data(cleaned_df, 'weather_data_1901_2019_full.csv')
                loader.generate_summary_statistics(cleaned_df)
        else:
            print("   å·²å–æ¶ˆå¤„ç†å…¨éƒ¨æ•°æ®")
    
    print("\nâœ… æ•°æ®å¤„ç†å®Œæˆï¼")


if __name__ == "__main__":
    main()
