"""
æ•°æ®åŠ è½½å’Œå¤„ç†æ¨¡å—
è´Ÿè´£å¤„ç† NOAA Global Surface Summary of Day æ•°æ®é›†
è¾“å‡ºæ¸…æ´—åçš„ CSV æ–‡ä»¶ä¾› HMM å’Œ Baseline æ¨¡å—ä½¿ç”¨
"""

import os
import gzip
import tarfile
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
import warnings
warnings.filterwarnings('ignore')


class GSODDataLoader:
    """NOAA GSOD æ•°æ®åŠ è½½å’Œå¤„ç†å™¨"""

    def __init__(
        self,
        data_root=None,
        output_dir=None,
        n_bins=5,
        discretize=True,
        station_list_csv=None,   # æ–°å¢ï¼šåªå¤„ç†è¿™äº›ç«™ç‚¹
        time_aggregation='daily',   # æ–°å¢ï¼šæ—¶é—´èšåˆæ–¹å¼ ('daily', 'monthly', 'quarterly', 'yearly')
    ):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨

        Args:
            data_root: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            n_bins: ç¦»æ•£åŒ–çš„ç»„æ•°ï¼ˆé»˜è®¤5ç»„ï¼Œæ‰€æœ‰è¿ç»­ç‰¹å¾ä½¿ç”¨ç›¸åŒç»„æ•°ï¼‰
            discretize: æ˜¯å¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ï¼ˆé»˜è®¤Trueï¼‰
            station_list_csv: ç«™ç‚¹åˆ—è¡¨ CSV è·¯å¾„ï¼ˆåŒ…å« USAFã€WBAN ç­‰ï¼‰ï¼Œ
                              è‹¥ä¸º Noneï¼Œåˆ™é»˜è®¤ä½¿ç”¨ isd-history.csv å…¨éƒ¨ç«™ç‚¹
            time_aggregation: æ—¶é—´èšåˆæ–¹å¼
                - 'daily': ä¿æŒæ¯æ—¥æ•°æ®ï¼ˆé»˜è®¤ï¼‰
                - 'monthly': èšåˆä¸ºæœˆå¹³å‡
                - 'quarterly': èšåˆä¸ºå­£åº¦å¹³å‡
                - 'yearly': èšåˆä¸ºå¹´å¹³å‡
        """
        if data_root is None:
            # è‡ªåŠ¨æ£€æµ‹æ•°æ®è·¯å¾„
            current_file = Path(__file__).resolve()
            proj_root = current_file.parent.parent.parent
            data_root = proj_root / "kaggle_data" / "datasets" / "noaa" / \
                        "noaa-global-surface-summary-of-the-day" / "versions" / "2"

        self.data_root = Path(data_root)
        self.gsod_dir = self.data_root / "gsod_all_years"

        # ç«™ç‚¹å…ƒæ•°æ® CSVï¼šå¯ä»¥æ˜¯å®Œæ•´ isd-history.csvï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ ç­›å¥½çš„é‚£ä»½
        if station_list_csv is not None:
            self.station_info_path = Path(station_list_csv)
        else:
            # ä½¿ç”¨ NOAA æä¾›çš„å®Œæ•´ç«™ç‚¹å†å²è®°å½•
            self.station_info_path = self.data_root / "isd-history.csv"

        if output_dir is None:
            output_dir = Path(__file__).parent / "processed"
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ç¦»æ•£åŒ–å‚æ•°
        self.n_bins = n_bins
        self.discretize = discretize
        
        # æ—¶é—´èšåˆå‚æ•°
        if time_aggregation not in ['daily', 'monthly', 'quarterly', 'yearly']:
            raise ValueError("time_aggregation å¿…é¡»æ˜¯ 'daily', 'monthly', 'quarterly' æˆ– 'yearly'")
        self.time_aggregation = time_aggregation

        # æ•°æ®æ ¼å¼å®šä¹‰ï¼ˆåŸºäº GSOD æ–‡æ¡£ï¼‰
        self.missing_values = {
            'TEMP': 9999.9,
            'DEWP': 9999.9,
            'SLP': 9999.9,
            'STP': 9999.9,
            'VISIB': 999.9,
            'WDSP': 999.9,
            'MXSPD': 999.9,
            'GUST': 999.9,
            'MAX': 9999.9,
            'MIN': 9999.9,
            'PRCP': 99.99,
            'SNDP': 999.9
        }

        # è¿ç»­ç‰¹å¾åˆ—è¡¨ï¼ˆéœ€è¦å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–çš„ç‰¹å¾ï¼‰
        self.continuous_features = [
            'mean_temp', 'dew_point', 'max_temp', 'min_temp',
            'sea_level_pressure', 'station_pressure',
            'visibility', 'wind_speed', 'max_wind_speed', 'wind_gust',
            'precipitation', 'snow_depth'
        ]

        # è¿™é‡Œå…ˆå ä¸€ä¸ªå±æ€§ï¼Œload_station_metadata é‡Œä¼šçœŸæ­£å¡«å……
        self.target_site_ids = None

    def load_station_metadata(self):
        """
        åŠ è½½ç«™ç‚¹å…ƒæ•°æ®ï¼ˆå¯ä»¥æ˜¯å®Œæ•´ isd-history.csvï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ ç­›å¥½çš„é‚£ä»½ï¼‰

        è¿”å›:
            df_meta: åŒ…å«è‡³å°‘ USAF, WBAN, site_id çš„ DataFrame
        """
        print("åŠ è½½ç«™ç‚¹å…ƒæ•°æ®...")
        df_raw = pd.read_csv(self.station_info_path)

        # ç»Ÿä¸€åˆ—åæ ¼å¼ï¼šå»ç©ºæ ¼ï¼Œå»å¼•å·ï¼Œå¤§å†™
        col_map = {c: c.strip().replace('"', '') for c in df_raw.columns}
        df_raw = df_raw.rename(columns=col_map)
        upper = {c.upper(): c for c in df_raw.columns}

        def get_col(*names):
            for n in names:
                if n in upper:
                    return upper[n]
            return None

        usaf_col = get_col("USAF")
        wban_col = get_col("WBAN")

        if usaf_col is None or wban_col is None:
            raise ValueError("ç«™ç‚¹åˆ—è¡¨ CSV ä¸­ç¼ºå°‘ USAF æˆ– WBAN åˆ—")

        name_col = get_col("STATION NAME", "NAME")
        ctry_col = get_col("CTRY", "COUNTRY")
        state_col = get_col("STATE", "ST")
        icao_col = get_col("ICAO")
        lat_col = get_col("LAT")
        lon_col = get_col("LON")
        elev_col = get_col("ELEV(M)", "ELEV", "ELEV(M)")
        begin_col = get_col("BEGIN")
        end_col = get_col("END")
        years_col = get_col("YEARS")

        df_meta = pd.DataFrame()
        df_meta["USAF"] = df_raw[usaf_col].astype(str).str.strip()
        df_meta["WBAN"] = df_raw[wban_col].astype(str).str.strip()

        if name_col:
            df_meta["Name"] = df_raw[name_col].astype(str).str.strip()
        if ctry_col:
            df_meta["Country"] = df_raw[ctry_col].astype(str).str.strip()
        if state_col:
            df_meta["State"] = df_raw[state_col].astype(str).str.strip()
        if icao_col:
            df_meta["ICAO"] = df_raw[icao_col].astype(str).str.strip()
        if lat_col:
            df_meta["LAT"] = df_raw[lat_col]
        if lon_col:
            df_meta["LON"] = df_raw[lon_col]
        if elev_col:
            df_meta["Elev(m)"] = df_raw[elev_col]
        if begin_col:
            df_meta["Begin"] = df_raw[begin_col].astype(str).str.strip()
        if end_col:
            df_meta["End"] = df_raw[end_col].astype(str).str.strip()
        if years_col:
            df_meta["Years"] = df_raw[years_col]

        # åˆ›å»º site_id (USAF-WBAN)
        df_meta["site_id"] = df_meta["USAF"].str.zfill(6) + "-" + df_meta["WBAN"].str.zfill(5)

        # ä¿å­˜ç›®æ ‡ç«™ç‚¹é›†åˆï¼Œç”¨äºåç»­è¿‡æ»¤ tar å†…æ–‡ä»¶
        self.target_site_ids = set(df_meta["site_id"].unique())
        print(f"   ç«™ç‚¹æ€»æ•°: {len(df_meta)}ï¼Œç›®æ ‡ site_id æ•°: {len(self.target_site_ids)}")

        return df_meta

    def parse_gsod_file(self, filepath):
        """
        è§£æå•ä¸ª GSOD æ–‡ä»¶ï¼ˆä½¿ç”¨å›ºå®šå®½åº¦æ ¼å¼ï¼‰

        Args:
            filepath: æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ .gz æˆ– .op æ–‡ä»¶ï¼‰

        Returns:
            DataFrame: è§£æåçš„æ•°æ®
        """
        try:
            colspecs = [
                (0, 6),     # STN
                (7, 12),    # WBAN
                (14, 22),   # YEARMODA
                (24, 30),   # TEMP
                (31, 33),   # TEMP_COUNT
                (35, 41),   # DEWP
                (42, 44),   # DEWP_COUNT
                (46, 52),   # SLP
                (53, 55),   # SLP_COUNT
                (57, 63),   # STP
                (64, 66),   # STP_COUNT
                (68, 73),   # VISIB
                (74, 76),   # VISIB_COUNT
                (78, 83),   # WDSP
                (84, 86),   # WDSP_COUNT
                (88, 93),   # MXSPD
                (95, 100),  # GUST
                (102, 108), # MAX
                (108, 109), # MAX_FLAG
                (110, 116), # MIN
                (116, 117), # MIN_FLAG
                (118, 123), # PRCP
                (123, 124), # PRCP_FLAG
                (125, 130), # SNDP
                (132, 138)  # FRSHTT
            ]

            names = [
                'STN---', 'WBAN', 'YEARMODA', 'TEMP', 'TEMP_COUNT', 'DEWP', 'DEWP_COUNT',
                'SLP', 'SLP_COUNT', 'STP', 'STP_COUNT', 'VISIB', 'VISIB_COUNT',
                'WDSP', 'WDSP_COUNT', 'MXSPD', 'GUST', 'MAX', 'MAX_FLAG',
                'MIN', 'MIN_FLAG', 'PRCP', 'PRCP_FLAG', 'SNDP', 'FRSHTT'
            ]

            if str(filepath).endswith('.gz'):
                with gzip.open(filepath, 'rt') as f:
                    df = pd.read_fwf(f, colspecs=colspecs, names=names, skiprows=1)
            else:
                df = pd.read_fwf(filepath, colspecs=colspecs, names=names, skiprows=1)

            return df
        except Exception as e:
            print(f"   è§£ææ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return None

    def _filter_members_by_site_id(self, members):
        """
        æ ¹æ® target_site_idsï¼Œç”¨æ–‡ä»¶åé‡Œçš„ USAF-WBAN è¿‡æ»¤ tar æˆå‘˜
        """
        if not self.target_site_ids:
            return members

        filtered = []
        for m in members:
            base = os.path.basename(m.name)
            if not (base.endswith(".op") or base.endswith(".op.gz")):
                continue

            # å¸¸è§æ–‡ä»¶åæ ¼å¼: "010010-99999-2010.op.gz"
            parts = base.split('-')
            if len(parts) < 2:
                continue

            stn = parts[0]
            wban_part = parts[1]  # ä¾‹å¦‚ "99999-2010.op.gz"
            wban = wban_part.split('.')[0].split('_')[0]
            # å»æ‰å¯èƒ½å¸¦çš„å¹´ä»½éƒ¨åˆ†
            wban = wban.split('-')[0]

            site_id = f"{stn.zfill(6)}-{wban.zfill(5)}"
            if site_id in self.target_site_ids:
                filtered.append(m)

        return filtered

    def process_year_data(self, year, max_stations=None):
        """
        å¤„ç†æŒ‡å®šå¹´ä»½çš„æ•°æ®

        Args:
            year: å¹´ä»½ï¼ˆå¦‚ 2015ï¼‰
            max_stations: æœ€å¤§å¤„ç†ç«™ç‚¹æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼ŒNoneè¡¨ç¤ºä¸é¢å¤–é™åˆ¶ï¼‰

        Returns:
            DataFrame: å¤„ç†åçš„æ•°æ®
        """
        print(f"\nå¤„ç† {year} å¹´æ•°æ®...")

        tar_path = self.gsod_dir / f"gsod_{year}.tar"
        if not tar_path.exists():
            print(f"   æ–‡ä»¶ä¸å­˜åœ¨: {tar_path}")
            return None

        all_data = []

        with tarfile.open(tar_path, 'r') as tar:
            members = tar.getmembers()

            # ä½¿ç”¨ç«™ç‚¹åˆ—è¡¨è¿‡æ»¤ tar æˆå‘˜
            members = self._filter_members_by_site_id(members)
            if not members:
                print("   è¯¥å¹´ä»½ä¸­æ²¡æœ‰åŒ¹é…ç›®æ ‡ç«™ç‚¹çš„æ–‡ä»¶")
                return None

            if max_stations is not None:
                members = members[:max_stations]

            print(f"   éœ€è¦å¤„ç†çš„ç«™ç‚¹æ–‡ä»¶æ•°: {len(members)}")

            for member in tqdm(members, desc=f"   è§£æ {year}"):
                if not (member.name.endswith('.op') or member.name.endswith('.op.gz')):
                    continue

                f = tar.extractfile(member)
                if f is None:
                    continue

                try:
                    if member.name.endswith('.gz'):
                        content = gzip.decompress(f.read()).decode('utf-8')
                    else:
                        content = f.read().decode('utf-8')

                    from io import StringIO
                    colspecs = [
                        (0, 6), (7, 12), (14, 22),
                        (24, 30), (31, 33),
                        (35, 41), (42, 44),
                        (46, 52), (53, 55),
                        (57, 63), (64, 66),
                        (68, 73), (74, 76),
                        (78, 83), (84, 86),
                        (88, 93), (95, 100),
                        (102, 108), (108, 109),
                        (110, 116), (116, 117),
                        (118, 123), (123, 124),
                        (125, 130), (132, 138)
                    ]
                    names = [
                        'STN---', 'WBAN', 'YEARMODA', 'TEMP', 'TEMP_COUNT', 'DEWP', 'DEWP_COUNT',
                        'SLP', 'SLP_COUNT', 'STP', 'STP_COUNT', 'VISIB', 'VISIB_COUNT',
                        'WDSP', 'WDSP_COUNT', 'MXSPD', 'GUST', 'MAX', 'MAX_FLAG',
                        'MIN', 'MIN_FLAG', 'PRCP', 'PRCP_FLAG', 'SNDP', 'FRSHTT'
                    ]
                    df = pd.read_fwf(StringIO(content), colspecs=colspecs, names=names, skiprows=1)
                    if len(df) > 0:
                        all_data.append(df)
                except Exception:
                    continue

        if not all_data:
            print("   æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ•°æ®")
            return None

        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"   æˆåŠŸå¤„ç† {len(combined_df)} è¡Œæ•°æ®ï¼Œæ¥è‡ª {len(all_data)} ä¸ªç«™ç‚¹æ–‡ä»¶")

        return combined_df

    def clean_and_transform(self, df):
        """
        æ¸…æ´—å’Œè½¬æ¢æ•°æ®
        """
        print("\næ¸…æ´—å’Œè½¬æ¢æ•°æ®...")

        df['STN---'] = df['STN---'].astype(str).str.zfill(6)
        df['WBAN'] = df['WBAN'].astype(str).str.zfill(5)
        df['site_id'] = df['STN---'] + '-' + df['WBAN']

        df['YEARMODA'] = df['YEARMODA'].astype(str).str.zfill(8)
        df['date'] = pd.to_datetime(df['YEARMODA'], format='%Y%m%d', errors='coerce')

        invalid_dates = df['date'].isna().sum()
        if invalid_dates > 0:
            print(f"   åˆ é™¤ {invalid_dates} è¡Œæ— æ•ˆæ—¥æœŸæ•°æ®")
            df = df.dropna(subset=['date'])

        for col, missing_val in self.missing_values.items():
            if col in df.columns:
                df[col] = df[col].replace(missing_val, np.nan)

        if 'FRSHTT' in df.columns:
            frshtt_str = df['FRSHTT'].astype(str).str.zfill(6)
            df['fog'] = frshtt_str.str[0].astype(int)
            df['rain'] = frshtt_str.str[1].astype(int)
            df['snow'] = frshtt_str.str[2].astype(int)
            df['hail'] = frshtt_str.str[3].astype(int)
            df['thunder'] = frshtt_str.str[4].astype(int)
            df['tornado'] = frshtt_str.str[5].astype(int)

        feature_cols = {
            'site_id': 'site_id',
            'date': 'date',
            'TEMP': 'mean_temp',
            'DEWP': 'dew_point',
            'SLP': 'sea_level_pressure',
            'STP': 'station_pressure',
            'VISIB': 'visibility',
            'WDSP': 'wind_speed',
            'MXSPD': 'max_wind_speed',
            'GUST': 'wind_gust',
            'MAX': 'max_temp',
            'MIN': 'min_temp',
            'PRCP': 'precipitation',
            'SNDP': 'snow_depth',
            'fog': 'fog',
            'rain': 'rain',
            'snow': 'snow',
            'hail': 'hail',
            'thunder': 'thunder',
            'tornado': 'tornado'
        }

        available_cols = {k: v for k, v in feature_cols.items() if k in df.columns}
        df_cleaned = df[list(available_cols.keys())].rename(columns=available_cols)

        df_cleaned = df_cleaned.sort_values(['site_id', 'date']).reset_index(drop=True)

        print(f"   æ¸…æ´—å®Œæˆï¼Œä¿ç•™ {len(df_cleaned)} è¡Œï¼Œ{len(df_cleaned.columns)} åˆ—ç‰¹å¾")

        # 1. å…ˆå¡«å……ç¼ºå¤±å€¼ï¼ˆåœ¨æ—¶é—´èšåˆä¹‹å‰ï¼Œä¿è¯æ’å€¼æ•ˆæœï¼‰
        df_cleaned = self.fill_missing_values(df_cleaned)
        
        # 2. è¿›è¡Œæ—¶é—´èšåˆï¼ˆå¦‚æœéœ€è¦ï¼‰
        if self.time_aggregation == 'monthly':
            df_cleaned = self.aggregate_to_monthly(df_cleaned)
        elif self.time_aggregation == 'quarterly':
            df_cleaned = self.aggregate_to_quarterly(df_cleaned)
        elif self.time_aggregation == 'yearly':
            df_cleaned = self.aggregate_to_yearly(df_cleaned)
        # å¦‚æœæ˜¯ 'daily'ï¼Œåˆ™ä¸åšèšåˆ
        
        # 3. è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        if self.discretize:
            df_cleaned = self.normalize_and_discretize(df_cleaned)

        return df_cleaned

    def fill_missing_values(self, df):
        """
        å¡«å……ç¼ºå¤±å€¼ï¼ˆæŒ‰ç«™ç‚¹è¿›è¡Œæ—¶é—´åºåˆ—æ’å€¼ï¼‰
        
        ç­–ç•¥ï¼š
        1. å¯¹æ¯ä¸ªç«™ç‚¹çš„è¿ç»­ç‰¹å¾ï¼Œä½¿ç”¨çº¿æ€§æ’å€¼
        2. å¦‚æœå¼€å¤´/ç»“å°¾ä»æœ‰NaNï¼Œä½¿ç”¨å‰å‘/åå‘å¡«å……
        3. å¦‚æœæ•´åˆ—éƒ½æ˜¯NaNï¼Œä½¿ç”¨å…¨å±€å‡å€¼
        4. äºŒè¿›åˆ¶ç‰¹å¾ç”¨0å¡«å……ï¼ˆè¡¨ç¤ºäº‹ä»¶æœªå‘ç”Ÿï¼‰
        """
        print("\nå¡«å……ç¼ºå¤±å€¼...")
        
        df_filled = df.copy()
        
        # ç»Ÿè®¡åŸå§‹ç¼ºå¤±æƒ…å†µ
        original_na_counts = {}
        for feature in self.continuous_features:
            if feature in df_filled.columns:
                original_na_counts[feature] = df_filled[feature].isna().sum()
        
        # æŒ‰ç«™ç‚¹åˆ†ç»„å¤„ç†è¿ç»­ç‰¹å¾
        print("   å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œæ—¶é—´åºåˆ—æ’å€¼...")
        for site_id in tqdm(df_filled['site_id'].unique(), desc="   å¤„ç†ç«™ç‚¹", leave=False):
            site_mask = df_filled['site_id'] == site_id
            
            for feature in self.continuous_features:
                if feature not in df_filled.columns:
                    continue
                
                # è·å–è¯¥ç«™ç‚¹è¯¥ç‰¹å¾çš„æ•°æ®
                site_feature_data = df_filled.loc[site_mask, feature]
                
                if site_feature_data.isna().all():
                    # å¦‚æœè¯¥ç«™ç‚¹è¯¥ç‰¹å¾å…¨æ˜¯NaNï¼Œè·³è¿‡ï¼ˆåé¢ç”¨å…¨å±€å‡å€¼å¡«å……ï¼‰
                    continue
                
                # 1. çº¿æ€§æ’å€¼ï¼ˆåŸºäºæ—¶é—´é¡ºåºï¼‰
                interpolated = site_feature_data.interpolate(method='linear', limit_direction='both')
                
                # 2. å‰å‘å¡«å……ï¼ˆå¤„ç†å¼€å¤´çš„NaNï¼‰
                interpolated = interpolated.fillna(method='ffill')
                
                # 3. åå‘å¡«å……ï¼ˆå¤„ç†ç»“å°¾çš„NaNï¼‰
                interpolated = interpolated.fillna(method='bfill')
                
                # æ›´æ–°æ•°æ®
                df_filled.loc[site_mask, feature] = interpolated
        
        # 4. å¦‚æœä»æœ‰NaNï¼ˆæ•´ä¸ªç«™ç‚¹éƒ½ç¼ºå¤±æˆ–æŸäº›ç‰¹æ®Šæƒ…å†µï¼‰ï¼Œä½¿ç”¨å…¨å±€å‡å€¼
        for feature in self.continuous_features:
            if feature in df_filled.columns:
                remaining_na = df_filled[feature].isna().sum()
                if remaining_na > 0:
                    global_mean = df_filled[feature].mean()
                    if pd.notna(global_mean):
                        df_filled[feature] = df_filled[feature].fillna(global_mean)
                        print(f"   âš ï¸  {feature}: ç”¨å…¨å±€å‡å€¼ {global_mean:.2f} å¡«å…… {remaining_na} ä¸ªå‰©ä½™NaN")
                    else:
                        # å¦‚æœå…¨å±€å‡å€¼éƒ½æ˜¯NaNï¼ˆæ•´åˆ—éƒ½ç¼ºå¤±ï¼‰ï¼Œå¡«å……ä¸º0
                        df_filled[feature] = df_filled[feature].fillna(0)
                        print(f"   âš ï¸  {feature}: å…¨éƒ¨ç¼ºå¤±ï¼Œç”¨0å¡«å……")
        
        # 5. äºŒè¿›åˆ¶ç‰¹å¾ï¼ˆå¤©æ°”äº‹ä»¶ï¼‰ï¼šNaNå¡«å……ä¸º0ï¼ˆè¡¨ç¤ºæœªå‘ç”Ÿï¼‰
        binary_features = ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']
        for feature in binary_features:
            if feature in df_filled.columns:
                na_count = df_filled[feature].isna().sum()
                if na_count > 0:
                    df_filled[feature] = df_filled[feature].fillna(0)
        
        # æ±‡æ€»å¡«å……ç»“æœ
        print(f"\n   ç¼ºå¤±å€¼å¡«å……ç»“æœï¼š")
        for feature in self.continuous_features:
            if feature in df_filled.columns and feature in original_na_counts:
                original_na = original_na_counts[feature]
                remaining_na = df_filled[feature].isna().sum()
                if original_na > 0:
                    print(f"      {feature:25s}: {original_na:6d} â†’ {remaining_na:6d} NaN")
        
        print(f"   âœ… ç¼ºå¤±å€¼å¡«å……å®Œæˆ")
        
        return df_filled

    def normalize_and_discretize(self, df):
        """
        å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–
        """
        print("\nå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–è¿ç»­ç‰¹å¾...")
        print(f"   ç¦»æ•£åŒ–ç»„æ•°: {self.n_bins}")

        df_processed = df.copy()
        normalization_info = {}

        for feature in self.continuous_features:
            if feature not in df_processed.columns:
                continue

            valid_mask = df_processed[feature].notna()
            valid_data = df_processed.loc[valid_mask, feature]

            if len(valid_data) == 0:
                print(f"   {feature}: å…¨éƒ¨ç¼ºå¤±ï¼Œè·³è¿‡")
                continue

            min_val = valid_data.min()
            max_val = valid_data.max()

            if max_val > min_val:
                normalized = (valid_data - min_val) / (max_val - min_val)
                discretized = pd.cut(
                    normalized,
                    bins=self.n_bins,
                    labels=range(self.n_bins),
                    include_lowest=True
                )
                discretized = discretized.astype(float)

                df_processed[f'{feature}_raw'] = df_processed[feature]
                df_processed.loc[valid_mask, feature] = discretized

                normalization_info[feature] = {
                    'min': min_val,
                    'max': max_val,
                    'bins': self.n_bins,
                    'unique_values': len(valid_data.unique())
                }

                print(f"   {feature:25s}: [{min_val:.2f}, {max_val:.2f}] -> [0, {self.n_bins-1}]")
            else:
                print(f"   {feature}: å€¼èŒƒå›´ä¸º 0ï¼Œè·³è¿‡ç¦»æ•£åŒ–")

        info_path = self.output_dir / "normalization_info.txt"
        with open(info_path, 'w') as f:
            f.write("è¿ç»­ç‰¹å¾å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ä¿¡æ¯\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ç¦»æ•£åŒ–ç»„æ•°: {self.n_bins}\n")
            f.write("ç¦»æ•£åŒ–æ–¹æ³•: ç­‰å®½åˆ†ç®±\n")
            f.write(f"ç»„æ ‡ç­¾: 0, 1, ..., {self.n_bins-1}\n\n")

            for feature, info in normalization_info.items():
                f.write(f"\n{feature}:\n")
                f.write(f"  åŸå§‹èŒƒå›´: [{info['min']:.2f}, {info['max']:.2f}]\n")
                f.write(f"  å”¯ä¸€å€¼æ•°: {info['unique_values']}\n")
                f.write(f"  ç¦»æ•£åŒ–å: {info['bins']} ç»„ (0-{info['bins']-1})\n")

        print(f"\n   å½’ä¸€åŒ–ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
        print(f"   ç¦»æ•£åŒ–å®Œæˆ: {len(normalization_info)} ä¸ªè¿ç»­ç‰¹å¾")

        return df_processed

    def aggregate_to_quarterly(self, df):
        """
        å°†æ¯æ—¥æ•°æ®èšåˆä¸ºå­£åº¦å¹³å‡æ•°æ®
        
        å­£åº¦åˆ’åˆ†ï¼š
        - Q1: 1-3æœˆï¼ˆå†¬æ˜¥ï¼‰
        - Q2: 4-6æœˆï¼ˆæ˜¥å¤ï¼‰
        - Q3: 7-9æœˆï¼ˆå¤ç§‹ï¼‰
        - Q4: 10-12æœˆï¼ˆç§‹å†¬ï¼‰
        
        Args:
            df: åŒ…å«æ¯æ—¥è§‚æµ‹çš„ DataFrame
            
        Returns:
            DataFrame: å­£åº¦å¹³å‡æ•°æ®
        """
        print("\nèšåˆä¸ºå­£åº¦å¹³å‡æ•°æ®...")
        
        # æå–å¹´ä»½å’Œå­£åº¦ä¿¡æ¯
        df['year'] = df['date'].dt.year
        df['quarter'] = df['date'].dt.quarter  # Pandas è‡ªåŠ¨è®¡ç®—å­£åº¦ (1-4)
        
        # å®šä¹‰èšåˆè§„åˆ™
        agg_dict = {}
        
        # è¿ç»­ç‰¹å¾ï¼šè®¡ç®—å¹³å‡å€¼
        for feature in self.continuous_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'
                # å¦‚æœæœ‰åŸå§‹å€¼åˆ—ï¼Œä¹Ÿè®¡ç®—å¹³å‡
                if f'{feature}_raw' in df.columns:
                    agg_dict[f'{feature}_raw'] = 'mean'
        
        # äºŒè¿›åˆ¶ç‰¹å¾ï¼ˆå¤©æ°”äº‹ä»¶ï¼‰ï¼šè®¡ç®—å‘ç”Ÿå¤©æ•°å æ¯”
        binary_features = ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']
        for feature in binary_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'
        
        # æ—¥æœŸï¼šå–è¯¥å­£åº¦çš„ç¬¬ä¸€å¤©ä½œä¸ºä»£è¡¨
        agg_dict['date'] = 'first'
        
        # æŒ‰ç«™ç‚¹ã€å¹´ã€å­£åº¦åˆ†ç»„èšåˆ
        print(f"   æŒ‰ç«™ç‚¹å’Œå­£åº¦åˆ†ç»„èšåˆ...")
        df_quarterly = df.groupby(['site_id', 'year', 'quarter'], as_index=False).agg(agg_dict)
        
        # å°†æ—¥æœŸè®¾ç½®ä¸ºæ¯å­£åº¦ç¬¬ä¸€ä¸ªæœˆçš„1å·
        # Q1: 1æœˆ1æ—¥, Q2: 4æœˆ1æ—¥, Q3: 7æœˆ1æ—¥, Q4: 10æœˆ1æ—¥
        quarter_to_month = {1: '01', 2: '04', 3: '07', 4: '10'}
        df_quarterly['date'] = pd.to_datetime(
            df_quarterly['year'].astype(str) + '-' + 
            df_quarterly['quarter'].map(quarter_to_month) + '-01'
        )
        
        # åˆ é™¤ä¸´æ—¶çš„ year å’Œ quarter åˆ—
        df_quarterly = df_quarterly.drop(columns=['year', 'quarter'])
        
        # é‡æ–°æ’åº
        df_quarterly = df_quarterly.sort_values(['site_id', 'date']).reset_index(drop=True)
        
        print(f"   èšåˆå®Œæˆ:")
        print(f"      åŸå§‹æ¯æ—¥æ•°æ®: {len(df):,} è¡Œ")
        print(f"      å­£åº¦å¹³å‡æ•°æ®: {len(df_quarterly):,} è¡Œ")
        print(f"      å¹³å‡æ¯ç«™ç‚¹å­£åº¦æ•°: {len(df_quarterly) / df_quarterly['site_id'].nunique():.1f}")
        
        return df_quarterly

    def aggregate_to_monthly(self, df):
        """
        å°†æ¯æ—¥æ•°æ®èšåˆä¸ºæœˆå¹³å‡æ•°æ®
        
        Args:
            df: åŒ…å«æ¯æ—¥è§‚æµ‹çš„ DataFrame
            
        Returns:
            DataFrame: æœˆå¹³å‡æ•°æ®
        """
        print("\nèšåˆä¸ºæœˆå¹³å‡æ•°æ®...")
        
        # æå–å¹´-æœˆä¿¡æ¯
        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        
        # å®šä¹‰èšåˆè§„åˆ™
        agg_dict = {}
        
        # è¿ç»­ç‰¹å¾ï¼šè®¡ç®—å¹³å‡å€¼
        for feature in self.continuous_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'
                # å¦‚æœæœ‰åŸå§‹å€¼åˆ—ï¼Œä¹Ÿè®¡ç®—å¹³å‡
                if f'{feature}_raw' in df.columns:
                    agg_dict[f'{feature}_raw'] = 'mean'
        
        # äºŒè¿›åˆ¶ç‰¹å¾ï¼ˆå¤©æ°”äº‹ä»¶ï¼‰ï¼šè®¡ç®—å‘ç”Ÿå¤©æ•°å æ¯”æˆ–æ€»æ¬¡æ•°
        binary_features = ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']
        for feature in binary_features:
            if feature in df.columns:
                # è®¡ç®—è¯¥æœˆè¯¥äº‹ä»¶å‘ç”Ÿçš„å¤©æ•°å æ¯”ï¼ˆ0-1ä¹‹é—´ï¼‰
                agg_dict[feature] = 'mean'
        
        # æ—¥æœŸï¼šå–è¯¥æœˆçš„ç¬¬ä¸€å¤©ä½œä¸ºä»£è¡¨
        agg_dict['date'] = 'first'
        
        # æŒ‰ç«™ç‚¹ã€å¹´ã€æœˆåˆ†ç»„èšåˆ
        print(f"   æŒ‰ç«™ç‚¹å’Œæœˆä»½åˆ†ç»„èšåˆ...")
        df_monthly = df.groupby(['site_id', 'year', 'month'], as_index=False).agg(agg_dict)
        
        # å°†æ—¥æœŸè®¾ç½®ä¸ºæ¯æœˆ1å·
        df_monthly['date'] = pd.to_datetime(
            df_monthly['year'].astype(str) + '-' + 
            df_monthly['month'].astype(str).str.zfill(2) + '-01'
        )
        
        # åˆ é™¤ä¸´æ—¶çš„ year å’Œ month åˆ—
        df_monthly = df_monthly.drop(columns=['year', 'month'])
        
        # é‡æ–°æ’åº
        df_monthly = df_monthly.sort_values(['site_id', 'date']).reset_index(drop=True)
        
        print(f"   èšåˆå®Œæˆ:")
        print(f"      åŸå§‹æ¯æ—¥æ•°æ®: {len(df):,} è¡Œ")
        print(f"      æœˆå¹³å‡æ•°æ®: {len(df_monthly):,} è¡Œ")
        print(f"      å¹³å‡æ¯ç«™ç‚¹æœˆæ•°: {len(df_monthly) / df_monthly['site_id'].nunique():.1f}")
        
        return df_monthly

    def aggregate_to_yearly(self, df):
        """å°†æ¯æ—¥æ•°æ®èšåˆä¸ºå¹´å¹³å‡æ•°æ®"""
        print("\nèšåˆä¸ºå¹´å¹³å‡æ•°æ®...")

        df['year'] = df['date'].dt.year

        agg_dict = {}
        for feature in self.continuous_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'
                if f'{feature}_raw' in df.columns:
                    agg_dict[f'{feature}_raw'] = 'mean'

        binary_features = ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']
        for feature in binary_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'

        agg_dict['date'] = 'first'

        print("   æŒ‰ç«™ç‚¹å’Œå¹´ä»½åˆ†ç»„èšåˆ...")
        df_yearly = df.groupby(['site_id', 'year'], as_index=False).agg(agg_dict)

        df_yearly['date'] = pd.to_datetime(df_yearly['year'].astype(str) + '-01-01')

        df_yearly = df_yearly.drop(columns=['year'])
        df_yearly = df_yearly.sort_values(['site_id', 'date']).reset_index(drop=True)

        print(f"   èšåˆå®Œæˆ:")
        print(f"      åŸå§‹æ¯æ—¥æ•°æ®: {len(df):,} è¡Œ")
        print(f"      å¹´å¹³å‡æ•°æ®: {len(df_yearly):,} è¡Œ")
        print(f"      å¹³å‡æ¯ç«™ç‚¹å¹´æ•°: {len(df_yearly) / df_yearly['site_id'].nunique():.1f}")

        return df_yearly

    def save_processed_data(self, df, filename='processed_weather_data.csv'):
        output_path = self.output_dir / filename
        df.to_csv(output_path, index=False)
        
        # æ—¶é—´ç²’åº¦æ˜¾ç¤º
        time_granularity_map = {
            'daily': 'æ¯æ—¥è§‚æµ‹',
            'monthly': 'æœˆå¹³å‡',
            'quarterly': 'å­£åº¦å¹³å‡',
            'yearly': 'å¹´å¹³å‡'
        }
        
        print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        print(f"   è¡Œæ•°: {len(df):,}")
        print(f"   åˆ—æ•°: {len(df.columns)}")
        print(f"   ç«™ç‚¹æ•°: {df['site_id'].nunique()}")
        print(f"   æ—¥æœŸèŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
        print(f"   æ—¶é—´ç²’åº¦: {time_granularity_map[self.time_aggregation]}")

        if self.discretize:
            raw_cols = [c for c in df.columns if c.endswith('_raw')]
            print(f"   ç¦»æ•£åŒ–ç‰¹å¾æ•°: {len(self.continuous_features)}")
            print(f"   ç¦»æ•£åŒ–ç»„æ•°: {self.n_bins}")
            print(f"   åŸå§‹å€¼åˆ—ï¼ˆ*_rawï¼‰: {len(raw_cols)} ä¸ª")

    def process_multiple_years(self, years, max_stations_per_year=None):
        all_years_data = []

        for year in years:
            df = self.process_year_data(year, max_stations=max_stations_per_year)
            if df is not None:
                all_years_data.append(df)

        if not all_years_data:
            print("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å¹´ä»½çš„æ•°æ®")
            return None

        print(f"\nåˆå¹¶ {len(all_years_data)} ä¸ªå¹´ä»½çš„æ•°æ®...")
        combined_df = pd.concat(all_years_data, ignore_index=True)
        cleaned_df = self.clean_and_transform(combined_df)
        return cleaned_df

    def generate_summary_statistics(self, df):
        print("\næ•°æ®æ‘˜è¦ç»Ÿè®¡:")
        print("=" * 60)

        print(f"\nåŸºæœ¬ä¿¡æ¯:")
        print(f"  æ€»è¡Œæ•°: {len(df):,}")
        print(f"  æ€»ç«™ç‚¹æ•°: {df['site_id'].nunique():,}")
        print(f"  æ—¥æœŸèŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")

        obs_per_site = df.groupby('site_id').size()
        print(f"\næ¯ä¸ªç«™ç‚¹çš„è§‚æµ‹å¤©æ•°:")
        print(f"  å¹³å‡: {obs_per_site.mean():.1f} å¤©")
        print(f"  ä¸­ä½æ•°: {obs_per_site.median():.1f} å¤©")
        print(f"  æœ€å°: {obs_per_site.min()} å¤©")
        print(f"  æœ€å¤§: {obs_per_site.max()} å¤©")

        print(f"\nç‰¹å¾ç¼ºå¤±ç‡:")
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            missing_rate = df[col].isna().sum() / len(df) * 100
            print(f"  {col}: {missing_rate:.2f}%")

        print("=" * 60)


def main():
    print("=" * 80)
    print("ğŸŒ¤ï¸  NOAA GSOD æ•°æ®å¤„ç†å·¥å…·")
    print("=" * 80)

    # 1. æ—¶é—´ç²’åº¦é€‰æ‹©
    print("\nã€1/4ã€‘é€‰æ‹©æ—¶é—´ç²’åº¦ï¼š")
    print("1. æ¯æ—¥è§‚æµ‹ï¼ˆDailyï¼‰- ä¿æŒåŸå§‹æ¯æ—¥æ•°æ®")
    print("2. æœˆå¹³å‡ï¼ˆMonthlyï¼‰- å°†æ¯æ—¥æ•°æ®èšåˆä¸ºæœˆå¹³å‡ï¼ˆæ•°æ®é‡çº¦å‡å°‘ 1/30ï¼‰")
    print("3. å­£åº¦å¹³å‡ï¼ˆQuarterlyï¼‰- å°†æ¯æ—¥æ•°æ®èšåˆä¸ºå­£åº¦å¹³å‡ï¼ˆæ•°æ®é‡çº¦å‡å°‘ 1/90ï¼‰")
    print("4. å¹´å¹³å‡ï¼ˆYearlyï¼‰- å°†æ¯æ—¥æ•°æ®èšåˆä¸ºå¹´å¹³å‡ï¼ˆæ•°æ®é‡çº¦å‡å°‘ 1/365ï¼‰")
    
    time_choice = input("\nè¯·é€‰æ‹© (1/2/3/4) [é»˜è®¤: 1]: ").strip() or "1"
    
    time_aggregation_map = {
        "1": "daily",
        "2": "monthly",
        "3": "quarterly",
        "4": "yearly"
    }
    time_aggregation = time_aggregation_map.get(time_choice, "daily")
    
    time_display = {
        "daily": "æ¯æ—¥è§‚æµ‹",
        "monthly": "æœˆå¹³å‡èšåˆ",
        "quarterly": "å­£åº¦å¹³å‡èšåˆ",
        "yearly": "å¹´å¹³å‡èšåˆ"
    }
    print(f"âœ“ å·²é€‰æ‹©: {time_display[time_aggregation]}")

    # 2. ç¦»æ•£åŒ–é€‰æ‹©
    print("\nã€2/4ã€‘æ˜¯å¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ï¼Ÿ")
    print("1. æ˜¯ - å½’ä¸€åŒ–åˆ° [0,1] å¹¶ç¦»æ•£åŒ–ä¸º N ç»„ï¼ˆæ¨èç”¨äº HMMï¼‰")
    print("2. å¦ - ä¿æŒåŸå§‹è¿ç»­å€¼")

    discretize_choice = input("\nè¯·é€‰æ‹© (1/2) [é»˜è®¤: 1]: ").strip() or "1"
    discretize = (discretize_choice == "1")

    n_bins = 5
    if discretize:
        n_bins_input = input(f"   ç¦»æ•£åŒ–ç»„æ•° (3-10) [é»˜è®¤: 5]: ").strip()
        if n_bins_input.isdigit():
            n_bins = int(n_bins_input)
            n_bins = max(3, min(10, n_bins))
        print(f"   âœ“ å°†ä½¿ç”¨ {n_bins} ç»„è¿›è¡Œç¦»æ•£åŒ–")
    else:
        print("   âœ“ ä¿æŒè¿ç»­å€¼")

    # 3. ç«™ç‚¹é€‰æ‹©
    print("\nã€3/4ã€‘é€‰æ‹©è¦å¤„ç†çš„ç«™ç‚¹ï¼š")
    station_csv_input = input(
        "   ç«™ç‚¹åˆ—è¡¨ CSV è·¯å¾„ï¼ˆåŒ…å« USAF, WBANï¼›ç•™ç©ºä½¿ç”¨é»˜è®¤ç«™ç‚¹åˆ—è¡¨ï¼‰: "
    ).strip() or None
    
    if station_csv_input:
        print(f"   âœ“ å°†ä½¿ç”¨è‡ªå®šä¹‰ç«™ç‚¹åˆ—è¡¨: {station_csv_input}")
    else:
        print("   âœ“ ä½¿ç”¨é»˜è®¤ç«™ç‚¹åˆ—è¡¨")

    loader = GSODDataLoader(
        n_bins=n_bins,
        discretize=discretize,
        station_list_csv=station_csv_input,
        time_aggregation=time_aggregation
    )

    # åŠ è½½ç«™ç‚¹å…ƒæ•°æ®ï¼ˆè¿™é‡Œä¼šç”Ÿæˆ target_site_idsï¼‰
    print("\n" + "=" * 80)
    loader.load_station_metadata()

    print("\næ£€æµ‹å¯ç”¨å¹´ä»½...")
    available_years = []
    for tar_file in sorted(loader.gsod_dir.glob("gsod_*.tar")):
        year = int(tar_file.stem.split('_')[1])
        available_years.append(year)

    print(f"   æ‰¾åˆ° {len(available_years)} ä¸ªå¹´ä»½: {available_years[0]} - {available_years[-1]}")

    # 4. å¹´ä»½èŒƒå›´é€‰æ‹©
    print("\nã€4/4ã€‘é€‰æ‹©å¤„ç†å“ªäº›å¹´ä»½çš„æ•°æ®ï¼š")
    print("1. å¿«é€Ÿæµ‹è¯•ï¼ˆ2015å¹´ï¼Œå‰50ä¸ªç›®æ ‡ç«™ç‚¹æ–‡ä»¶ï¼‰")
    print("2. å•å¹´å®Œæ•´ï¼ˆ2015å¹´ï¼Œæ‰€æœ‰ç›®æ ‡ç«™ç‚¹æ–‡ä»¶ï¼‰")
    print("3. è¿‘æœŸæ•°æ®ï¼ˆ1973-2019 å¹´ï¼Œæ‰€æœ‰ç›®æ ‡ç«™ç‚¹ï¼‰")
    print("4. å…¨éƒ¨æ•°æ®ï¼ˆ1901-2019 å¹´ï¼Œæ‰€æœ‰ç›®æ ‡ç«™ç‚¹ï¼‰")

    choice = input(f"\nè¯·é€‰æ‹© (1/2/3/4) [é»˜è®¤: 3]: ").strip() or "3"
    
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹å¤„ç†æ•°æ®...")
    print("=" * 80)

    # æ ¹æ®é€‰é¡¹ç”Ÿæˆæ–‡ä»¶ååç¼€
    time_suffix = time_aggregation  # 'daily', 'monthly', æˆ– 'quarterly'
    discrete_suffix = f"bins{n_bins}" if discretize else "continuous"
    
    if choice == "1":
        print("\nğŸ“ å¿«é€Ÿæµ‹è¯•æ¨¡å¼...")
        df = loader.process_year_data(2015, max_stations=50)
        if df is not None:
            cleaned_df = loader.clean_and_transform(df)
            filename = f'weather_2015_sample_{time_suffix}_{discrete_suffix}.csv'
            loader.save_processed_data(cleaned_df, filename)
            loader.generate_summary_statistics(cleaned_df)

    elif choice == "2":
        print("\nğŸ“ å¤„ç† 2015 å¹´å®Œæ•´æ•°æ®ï¼ˆä»…ç›®æ ‡ç«™ç‚¹ï¼‰...")
        df = loader.process_year_data(2015)
        if df is not None:
            cleaned_df = loader.clean_and_transform(df)
            filename = f'weather_2015_full_{time_suffix}_{discrete_suffix}.csv'
            loader.save_processed_data(cleaned_df, filename)
            loader.generate_summary_statistics(cleaned_df)

    elif choice == "3":
        print("\nğŸ“ å¤„ç†è¿‘æœŸæ•°æ®ï¼ˆ1973-2019ï¼Œç›®æ ‡ç«™ç‚¹ï¼‰...")
        years = [y for y in available_years if y >= 1973]
        print(f"   å°†å¤„ç† {len(years)} ä¸ªå¹´ä»½")
        cleaned_df = loader.process_multiple_years(years)
        if cleaned_df is not None:
            filename = f'weather_1973_2019_{time_suffix}_{discrete_suffix}.csv'
            loader.save_processed_data(cleaned_df, filename)
            loader.generate_summary_statistics(cleaned_df)

    elif choice == "4":
        print("\nğŸ“ å¤„ç†å…¨éƒ¨å†å²æ•°æ®ï¼ˆ1901-2019ï¼Œç›®æ ‡ç«™ç‚¹ï¼‰...")
        print(f"   âš ï¸  è­¦å‘Šï¼šå…¨éƒ¨æ•°æ®é‡å¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        confirm = input("   ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no) [no]: ").strip().lower()
        if confirm == "yes":
            cleaned_df = loader.process_multiple_years(available_years)
            if cleaned_df is not None:
                filename = f'weather_1901_2019_{time_suffix}_{discrete_suffix}.csv'
                loader.save_processed_data(cleaned_df, filename)
                loader.generate_summary_statistics(cleaned_df)
        else:
            print("   âœ— å·²å–æ¶ˆå¤„ç†å…¨éƒ¨æ•°æ®")

    print("\n" + "=" * 80)
    print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()
