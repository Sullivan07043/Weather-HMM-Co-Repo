"""
æ•°æ®åŠ è½½å’Œå¤„ç†æ¨¡å—
è´Ÿè´£å¤„ç† NOAA Global Surface Summary of Day æ•°æ®é›†
è¾“å‡ºæ¸…æ´—åçš„ CSV æ–‡ä»¶ä¾› HMM å’Œ Baseline æ¨¡å—ä½¿ç”¨
"""

import os
import gzip
import tarfile
import pandas as pd
import numpy as np
from pathlib import Path
from tqdm import tqdm
from scipy.signal import butter, filtfilt
from scipy.stats import linregress
import warnings
warnings.filterwarnings('ignore')


class GSODDataLoader:
    """NOAA GSOD æ•°æ®åŠ è½½å’Œå¤„ç†å™¨"""

    def __init__(
        self,
        data_root=None,
        output_dir=None,
        n_bins=5,
        discretize=True,
        station_list_csv=None,   # æ–°å¢ï¼šåªå¤„ç†è¿™äº›ç«™ç‚¹
        time_aggregation='daily',   # æ–°å¢ï¼šæ—¶é—´èšåˆæ–¹å¼ ('daily', 'monthly', 'quarterly', 'yearly')
        detrend=False,   # æ–°å¢ï¼šæ˜¯å¦å»è¶‹åŠ¿
        detrend_method='difference',   # æ–°å¢ï¼šå»è¶‹åŠ¿æ–¹æ³•
        complete_time_series=True,   # æ–°å¢ï¼šæ˜¯å¦ç¡®ä¿å®Œæ•´æ—¶é—´åºåˆ—
    ):
        """
        åˆå§‹åŒ–æ•°æ®åŠ è½½å™¨

        Args:
            data_root: æ•°æ®æ ¹ç›®å½•è·¯å¾„
            output_dir: è¾“å‡ºç›®å½•è·¯å¾„
            n_bins: ç¦»æ•£åŒ–çš„ç»„æ•°ï¼ˆé»˜è®¤5ç»„ï¼Œæ‰€æœ‰è¿ç»­ç‰¹å¾ä½¿ç”¨ç›¸åŒç»„æ•°ï¼‰
            discretize: æ˜¯å¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ï¼ˆé»˜è®¤Trueï¼‰
            station_list_csv: ç«™ç‚¹åˆ—è¡¨ CSV è·¯å¾„ï¼ˆåŒ…å« USAFã€WBAN ç­‰ï¼‰ï¼Œ
                              è‹¥ä¸º Noneï¼Œåˆ™é»˜è®¤ä½¿ç”¨ isd-history.csv å…¨éƒ¨ç«™ç‚¹
            time_aggregation: æ—¶é—´èšåˆæ–¹å¼
                - 'daily': ä¿æŒæ¯æ—¥æ•°æ®ï¼ˆé»˜è®¤ï¼‰
                - 'monthly': èšåˆä¸ºæœˆå¹³å‡
                - 'quarterly': èšåˆä¸ºå­£åº¦å¹³å‡
                - 'yearly': èšåˆä¸ºå¹´å¹³å‡
            detrend: æ˜¯å¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå»è¶‹åŠ¿å¤„ç†ï¼ˆé»˜è®¤Falseï¼‰
            detrend_method: å»è¶‹åŠ¿æ–¹æ³•
                - 'adaptive': è‡ªé€‚åº”æ–¹æ³•ï¼ˆæ¨èï¼‰- è‡ªåŠ¨æµ‹è¯•å¤šç§æ–¹æ³•å¹¶é€‰æ‹©æ•ˆæœæœ€å¥½çš„
                  åŒ…æ‹¬ï¼šçº¿æ€§å›å½’ã€äºŒæ¬¡å¤šé¡¹å¼ã€ä¸€é˜¶å·®åˆ†ã€é«˜é€šæ»¤æ³¢ï¼ˆButterworthï¼‰
                - 'difference': ä¸€é˜¶å·®åˆ†
                - 'linear': çº¿æ€§å»è¶‹åŠ¿
                - 'moving_average': ç§»åŠ¨å¹³å‡å»è¶‹åŠ¿
                - 'seasonal': å­£èŠ‚æ€§å·®åˆ†ï¼ˆé€‚åˆæœ‰æ˜æ˜¾å­£èŠ‚æ€§çš„æ•°æ®ï¼‰
            complete_time_series: æ˜¯å¦ç¡®ä¿æ¯ä¸ªç«™ç‚¹éƒ½æœ‰å®Œæ•´çš„æ—¶é—´åºåˆ—ï¼ˆé»˜è®¤Trueï¼‰
                - True: ä¸ºæ¯ä¸ªç«™ç‚¹åˆ›å»ºå®Œæ•´æ—¥æœŸèŒƒå›´ï¼Œç¼ºå¤±æ—¥æœŸç”¨æ’å€¼å¡«å……
                - False: åªå¡«å……å·²æœ‰è®°å½•ä¸­çš„ç¼ºå¤±å€¼ï¼Œä¸è¡¥å…¨ç¼ºå¤±çš„æ—¥æœŸ
        """
        if data_root is None:
            # è‡ªåŠ¨æ£€æµ‹æ•°æ®è·¯å¾„
            current_file = Path(__file__).resolve()
            proj_root = current_file.parent.parent.parent
            data_root = proj_root / "kaggle_data" / "datasets" / "noaa" / \
                        "noaa-global-surface-summary-of-the-day" / "versions" / "2"

        self.data_root = Path(data_root)
        self.gsod_dir = self.data_root / "gsod_all_years"

        # ç«™ç‚¹å…ƒæ•°æ® CSVï¼šå¯ä»¥æ˜¯å®Œæ•´ isd-history.csvï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ ç­›å¥½çš„é‚£ä»½
        if station_list_csv is not None:
            self.station_info_path = Path(station_list_csv)
        else:
            # ä½¿ç”¨ NOAA æä¾›çš„å®Œæ•´ç«™ç‚¹å†å²è®°å½•
            self.station_info_path = self.data_root / "isd-history.csv"

        if output_dir is None:
            output_dir = Path(__file__).parent / "processed"
        self.output_dir = Path(output_dir)
        self.output_dir.mkdir(parents=True, exist_ok=True)

        # ç¦»æ•£åŒ–å‚æ•°
        self.n_bins = n_bins
        self.discretize = discretize
        
        # æ—¶é—´èšåˆå‚æ•°
        if time_aggregation not in ['daily', 'monthly', 'quarterly', 'yearly']:
            raise ValueError("time_aggregation å¿…é¡»æ˜¯ 'daily', 'monthly', 'quarterly' æˆ– 'yearly'")
        self.time_aggregation = time_aggregation
        
        # å»è¶‹åŠ¿å‚æ•°
        self.detrend = detrend
        if detrend_method not in ['difference', 'linear', 'moving_average', 'seasonal', 'adaptive']:
            raise ValueError("detrend_method å¿…é¡»æ˜¯ 'difference', 'linear', 'moving_average', 'seasonal' æˆ– 'adaptive'")
        self.detrend_method = detrend_method
        
        # å®Œæ•´æ—¶é—´åºåˆ—å‚æ•°
        self.complete_time_series = complete_time_series

        # æ•°æ®æ ¼å¼å®šä¹‰ï¼ˆåŸºäº GSOD æ–‡æ¡£ï¼‰
        self.missing_values = {
            'TEMP': 9999.9,
            'DEWP': 9999.9,
            'SLP': 9999.9,
            'STP': 9999.9,
            'VISIB': 999.9,
            'WDSP': 999.9,
            'MXSPD': 999.9,
            'GUST': 999.9,
            'MAX': 9999.9,
            'MIN': 9999.9,
            'PRCP': 99.99,
            'SNDP': 999.9
        }

        # è¿ç»­ç‰¹å¾åˆ—è¡¨ï¼ˆéœ€è¦å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–çš„ç‰¹å¾ï¼‰
        self.continuous_features = [
            'mean_temp', 'dew_point', 'max_temp', 'min_temp',
            'sea_level_pressure', 'station_pressure',
            'visibility', 'wind_speed', 'max_wind_speed', 'wind_gust',
            'precipitation', 'snow_depth'
        ]
        
        # äºŒå…ƒç‰¹å¾åˆ—è¡¨ï¼ˆä¹Ÿéœ€è¦å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ï¼‰
        self.binary_features = ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']

        # è¿™é‡Œå…ˆå ä¸€ä¸ªå±æ€§ï¼Œload_station_metadata é‡Œä¼šçœŸæ­£å¡«å……
        self.target_site_ids = None

    def load_station_metadata(self):
        """
        åŠ è½½ç«™ç‚¹å…ƒæ•°æ®ï¼ˆå¯ä»¥æ˜¯å®Œæ•´ isd-history.csvï¼Œä¹Ÿå¯ä»¥æ˜¯ä½ ç­›å¥½çš„é‚£ä»½ï¼‰

        è¿”å›:
            df_meta: åŒ…å«è‡³å°‘ USAF, WBAN, site_id çš„ DataFrame
        """
        print("åŠ è½½ç«™ç‚¹å…ƒæ•°æ®...")
        df_raw = pd.read_csv(self.station_info_path)

        # ç»Ÿä¸€åˆ—åæ ¼å¼ï¼šå»ç©ºæ ¼ï¼Œå»å¼•å·ï¼Œå¤§å†™
        col_map = {c: c.strip().replace('"', '') for c in df_raw.columns}
        df_raw = df_raw.rename(columns=col_map)
        upper = {c.upper(): c for c in df_raw.columns}

        def get_col(*names):
            for n in names:
                if n in upper:
                    return upper[n]
            return None

        usaf_col = get_col("USAF")
        wban_col = get_col("WBAN")

        if usaf_col is None or wban_col is None:
            raise ValueError("ç«™ç‚¹åˆ—è¡¨ CSV ä¸­ç¼ºå°‘ USAF æˆ– WBAN åˆ—")

        name_col = get_col("STATION NAME", "NAME")
        ctry_col = get_col("CTRY", "COUNTRY")
        state_col = get_col("STATE", "ST")
        icao_col = get_col("ICAO")
        lat_col = get_col("LAT")
        lon_col = get_col("LON")
        elev_col = get_col("ELEV(M)", "ELEV", "ELEV(M)")
        begin_col = get_col("BEGIN")
        end_col = get_col("END")
        years_col = get_col("YEARS")

        df_meta = pd.DataFrame()
        df_meta["USAF"] = df_raw[usaf_col].astype(str).str.strip()
        df_meta["WBAN"] = df_raw[wban_col].astype(str).str.strip()

        if name_col:
            df_meta["Name"] = df_raw[name_col].astype(str).str.strip()
        if ctry_col:
            df_meta["Country"] = df_raw[ctry_col].astype(str).str.strip()
        if state_col:
            df_meta["State"] = df_raw[state_col].astype(str).str.strip()
        if icao_col:
            df_meta["ICAO"] = df_raw[icao_col].astype(str).str.strip()
        if lat_col:
            df_meta["LAT"] = df_raw[lat_col]
        if lon_col:
            df_meta["LON"] = df_raw[lon_col]
        if elev_col:
            df_meta["Elev(m)"] = df_raw[elev_col]
        if begin_col:
            df_meta["Begin"] = df_raw[begin_col].astype(str).str.strip()
        if end_col:
            df_meta["End"] = df_raw[end_col].astype(str).str.strip()
        if years_col:
            df_meta["Years"] = df_raw[years_col]

        # åˆ›å»º site_id (USAF-WBAN)
        df_meta["site_id"] = df_meta["USAF"].str.zfill(6) + "-" + df_meta["WBAN"].str.zfill(5)

        # ä¿å­˜ç›®æ ‡ç«™ç‚¹é›†åˆï¼Œç”¨äºåç»­è¿‡æ»¤ tar å†…æ–‡ä»¶
        self.target_site_ids = set(df_meta["site_id"].unique())
        print(f"   ç«™ç‚¹æ€»æ•°: {len(df_meta)}ï¼Œç›®æ ‡ site_id æ•°: {len(self.target_site_ids)}")

        return df_meta

    def parse_gsod_file(self, filepath):
        """
        è§£æå•ä¸ª GSOD æ–‡ä»¶ï¼ˆä½¿ç”¨å›ºå®šå®½åº¦æ ¼å¼ï¼‰

        Args:
            filepath: æ–‡ä»¶è·¯å¾„ï¼ˆå¯ä»¥æ˜¯ .gz æˆ– .op æ–‡ä»¶ï¼‰

        Returns:
            DataFrame: è§£æåçš„æ•°æ®
        """
        try:
            colspecs = [
                (0, 6),     # STN
                (7, 12),    # WBAN
                (14, 22),   # YEARMODA
                (24, 30),   # TEMP
                (31, 33),   # TEMP_COUNT
                (35, 41),   # DEWP
                (42, 44),   # DEWP_COUNT
                (46, 52),   # SLP
                (53, 55),   # SLP_COUNT
                (57, 63),   # STP
                (64, 66),   # STP_COUNT
                (68, 73),   # VISIB
                (74, 76),   # VISIB_COUNT
                (78, 83),   # WDSP
                (84, 86),   # WDSP_COUNT
                (88, 93),   # MXSPD
                (95, 100),  # GUST
                (102, 108), # MAX
                (108, 109), # MAX_FLAG
                (110, 116), # MIN
                (116, 117), # MIN_FLAG
                (118, 123), # PRCP
                (123, 124), # PRCP_FLAG
                (125, 130), # SNDP
                (132, 138)  # FRSHTT
            ]

            names = [
                'STN---', 'WBAN', 'YEARMODA', 'TEMP', 'TEMP_COUNT', 'DEWP', 'DEWP_COUNT',
                'SLP', 'SLP_COUNT', 'STP', 'STP_COUNT', 'VISIB', 'VISIB_COUNT',
                'WDSP', 'WDSP_COUNT', 'MXSPD', 'GUST', 'MAX', 'MAX_FLAG',
                'MIN', 'MIN_FLAG', 'PRCP', 'PRCP_FLAG', 'SNDP', 'FRSHTT'
            ]

            if str(filepath).endswith('.gz'):
                with gzip.open(filepath, 'rt') as f:
                    df = pd.read_fwf(f, colspecs=colspecs, names=names, skiprows=1)
            else:
                df = pd.read_fwf(filepath, colspecs=colspecs, names=names, skiprows=1)

            return df
        except Exception as e:
            print(f"   è§£ææ–‡ä»¶å¤±è´¥ {filepath}: {e}")
            return None

    def _filter_members_by_site_id(self, members):
        """
        æ ¹æ® target_site_idsï¼Œç”¨æ–‡ä»¶åé‡Œçš„ USAF-WBAN è¿‡æ»¤ tar æˆå‘˜
        """
        if not self.target_site_ids:
            return members

        filtered = []
        for m in members:
            base = os.path.basename(m.name)
            if not (base.endswith(".op") or base.endswith(".op.gz")):
                continue

            # å¸¸è§æ–‡ä»¶åæ ¼å¼: "010010-99999-2010.op.gz"
            parts = base.split('-')
            if len(parts) < 2:
                continue

            stn = parts[0]
            wban_part = parts[1]  # ä¾‹å¦‚ "99999-2010.op.gz"
            wban = wban_part.split('.')[0].split('_')[0]
            # å»æ‰å¯èƒ½å¸¦çš„å¹´ä»½éƒ¨åˆ†
            wban = wban.split('-')[0]

            site_id = f"{stn.zfill(6)}-{wban.zfill(5)}"
            if site_id in self.target_site_ids:
                filtered.append(m)

        return filtered

    def process_year_data(self, year, max_stations=None):
        """
        å¤„ç†æŒ‡å®šå¹´ä»½çš„æ•°æ®

        Args:
            year: å¹´ä»½ï¼ˆå¦‚ 2015ï¼‰
            max_stations: æœ€å¤§å¤„ç†ç«™ç‚¹æ•°ï¼ˆç”¨äºæµ‹è¯•ï¼ŒNoneè¡¨ç¤ºä¸é¢å¤–é™åˆ¶ï¼‰

        Returns:
            DataFrame: å¤„ç†åçš„æ•°æ®
        """
        print(f"\nå¤„ç† {year} å¹´æ•°æ®...")

        tar_path = self.gsod_dir / f"gsod_{year}.tar"
        if not tar_path.exists():
            print(f"   æ–‡ä»¶ä¸å­˜åœ¨: {tar_path}")
            return None

        all_data = []

        with tarfile.open(tar_path, 'r') as tar:
            members = tar.getmembers()

            # ä½¿ç”¨ç«™ç‚¹åˆ—è¡¨è¿‡æ»¤ tar æˆå‘˜
            members = self._filter_members_by_site_id(members)
            if not members:
                print("   è¯¥å¹´ä»½ä¸­æ²¡æœ‰åŒ¹é…ç›®æ ‡ç«™ç‚¹çš„æ–‡ä»¶")
                return None

            if max_stations is not None:
                members = members[:max_stations]

            print(f"   éœ€è¦å¤„ç†çš„ç«™ç‚¹æ–‡ä»¶æ•°: {len(members)}")

            for member in tqdm(members, desc=f"   è§£æ {year}"):
                if not (member.name.endswith('.op') or member.name.endswith('.op.gz')):
                    continue

                f = tar.extractfile(member)
                if f is None:
                    continue

                try:
                    if member.name.endswith('.gz'):
                        content = gzip.decompress(f.read()).decode('utf-8')
                    else:
                        content = f.read().decode('utf-8')

                    from io import StringIO
                    colspecs = [
                        (0, 6), (7, 12), (14, 22),
                        (24, 30), (31, 33),
                        (35, 41), (42, 44),
                        (46, 52), (53, 55),
                        (57, 63), (64, 66),
                        (68, 73), (74, 76),
                        (78, 83), (84, 86),
                        (88, 93), (95, 100),
                        (102, 108), (108, 109),
                        (110, 116), (116, 117),
                        (118, 123), (123, 124),
                        (125, 130), (132, 138)
                    ]
                    names = [
                        'STN---', 'WBAN', 'YEARMODA', 'TEMP', 'TEMP_COUNT', 'DEWP', 'DEWP_COUNT',
                        'SLP', 'SLP_COUNT', 'STP', 'STP_COUNT', 'VISIB', 'VISIB_COUNT',
                        'WDSP', 'WDSP_COUNT', 'MXSPD', 'GUST', 'MAX', 'MAX_FLAG',
                        'MIN', 'MIN_FLAG', 'PRCP', 'PRCP_FLAG', 'SNDP', 'FRSHTT'
                    ]
                    df = pd.read_fwf(StringIO(content), colspecs=colspecs, names=names, skiprows=1)
                    if len(df) > 0:
                        all_data.append(df)
                except Exception:
                    continue

        if not all_data:
            print("   æ²¡æœ‰æˆåŠŸè§£æä»»ä½•æ•°æ®")
            return None

        combined_df = pd.concat(all_data, ignore_index=True)
        print(f"   æˆåŠŸå¤„ç† {len(combined_df)} è¡Œæ•°æ®ï¼Œæ¥è‡ª {len(all_data)} ä¸ªç«™ç‚¹æ–‡ä»¶")

        return combined_df

    def clean_and_transform(self, df):
        """
        æ¸…æ´—å’Œè½¬æ¢æ•°æ®
        """
        print("\næ¸…æ´—å’Œè½¬æ¢æ•°æ®...")

        df['STN---'] = df['STN---'].astype(str).str.zfill(6)
        df['WBAN'] = df['WBAN'].astype(str).str.zfill(5)
        df['site_id'] = df['STN---'] + '-' + df['WBAN']

        df['YEARMODA'] = df['YEARMODA'].astype(str).str.zfill(8)
        df['date'] = pd.to_datetime(df['YEARMODA'], format='%Y%m%d', errors='coerce')

        invalid_dates = df['date'].isna().sum()
        if invalid_dates > 0:
            print(f"   åˆ é™¤ {invalid_dates} è¡Œæ— æ•ˆæ—¥æœŸæ•°æ®")
            df = df.dropna(subset=['date'])

        for col, missing_val in self.missing_values.items():
            if col in df.columns:
                df[col] = df[col].replace(missing_val, np.nan)

        if 'FRSHTT' in df.columns:
            frshtt_str = df['FRSHTT'].astype(str).str.zfill(6)
            df['fog'] = frshtt_str.str[0].astype(int)
            df['rain'] = frshtt_str.str[1].astype(int)
            df['snow'] = frshtt_str.str[2].astype(int)
            df['hail'] = frshtt_str.str[3].astype(int)
            df['thunder'] = frshtt_str.str[4].astype(int)
            df['tornado'] = frshtt_str.str[5].astype(int)

        feature_cols = {
            'site_id': 'site_id',
            'date': 'date',
            'TEMP': 'mean_temp',
            'DEWP': 'dew_point',
            'SLP': 'sea_level_pressure',
            'STP': 'station_pressure',
            'VISIB': 'visibility',
            'WDSP': 'wind_speed',
            'MXSPD': 'max_wind_speed',
            'GUST': 'wind_gust',
            'MAX': 'max_temp',
            'MIN': 'min_temp',
            'PRCP': 'precipitation',
            'SNDP': 'snow_depth',
            'fog': 'fog',
            'rain': 'rain',
            'snow': 'snow',
            'hail': 'hail',
            'thunder': 'thunder',
            'tornado': 'tornado'
        }

        available_cols = {k: v for k, v in feature_cols.items() if k in df.columns}
        df_cleaned = df[list(available_cols.keys())].rename(columns=available_cols)

        df_cleaned = df_cleaned.sort_values(['site_id', 'date']).reset_index(drop=True)

        print(f"   æ¸…æ´—å®Œæˆï¼Œä¿ç•™ {len(df_cleaned)} è¡Œï¼Œ{len(df_cleaned.columns)} åˆ—ç‰¹å¾")

        # ========================================================================
        # æ•°æ®å¤„ç†æµç¨‹ï¼ˆæŒ‰æ­£ç¡®é¡ºåºï¼‰
        # ========================================================================
        
        # æ­¥éª¤1: æ—¶é—´èšåˆï¼ˆå¦‚æœéœ€è¦ï¼‰
        # è¯´æ˜ï¼šå…ˆèšåˆå¯ä»¥å‡å°‘æ•°æ®é‡ï¼Œæé«˜åç»­å¤„ç†æ•ˆç‡
        if self.time_aggregation == 'monthly':
            print("\nã€æ­¥éª¤1/5ã€‘æ—¶é—´èšåˆ: èšåˆä¸ºæœˆå¹³å‡...")
            df_cleaned = self.aggregate_to_monthly(df_cleaned)
        elif self.time_aggregation == 'quarterly':
            print("\nã€æ­¥éª¤1/5ã€‘æ—¶é—´èšåˆ: èšåˆä¸ºå­£åº¦å¹³å‡...")
            df_cleaned = self.aggregate_to_quarterly(df_cleaned)
        elif self.time_aggregation == 'yearly':
            print("\nã€æ­¥éª¤1/5ã€‘æ—¶é—´èšåˆ: èšåˆä¸ºå¹´å¹³å‡...")
            df_cleaned = self.aggregate_to_yearly(df_cleaned)
        else:
            print("\nã€æ­¥éª¤1/5ã€‘æ—¶é—´èšåˆ: ä¿æŒæ¯æ—¥æ•°æ®ï¼Œè·³è¿‡èšåˆ")
        
        # æ­¥éª¤2: ç¡®ä¿å®Œæ•´æ—¶é—´åºåˆ—ï¼ˆå¡«è¡¥ç¼ºå¤±çš„æ—¥æœŸè®°å½•ï¼‰
        # è¯´æ˜ï¼šåœ¨èšåˆåè¿›è¡Œï¼Œç¡®ä¿æ¯ä¸ªç«™ç‚¹åœ¨èšåˆåçš„æ—¶é—´ç²’åº¦ä¸Šéƒ½æœ‰å®Œæ•´åºåˆ—
        if self.complete_time_series:
            print("\nã€æ­¥éª¤2/5ã€‘å®Œæ•´æ—¶é—´åºåˆ—: å¡«è¡¥ç¼ºå¤±çš„æ—¶é—´ç‚¹...")
            df_cleaned = self.ensure_complete_time_series(df_cleaned)
        else:
            print("\nã€æ­¥éª¤2/5ã€‘å®Œæ•´æ—¶é—´åºåˆ—: è·³è¿‡ï¼ˆä»…å¤„ç†å·²æœ‰è®°å½•ï¼‰")
        
        # æ­¥éª¤3: æ’å€¼å¡«å……ç¼ºå¤±å€¼
        # è¯´æ˜ï¼šåœ¨å®Œæ•´æ—¶é—´åºåˆ—åˆ›å»ºåï¼Œå¯¹æ‰€æœ‰ç¼ºå¤±å€¼è¿›è¡Œæ™ºèƒ½æ’å€¼
        print("\nã€æ­¥éª¤3/5ã€‘æ’å€¼å¡«å……: å¡«å……ç¼ºå¤±å€¼...")
        df_cleaned = self.fill_missing_values(df_cleaned)
        
        # æ­¥éª¤4: å»è¶‹åŠ¿å¤„ç†ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # è¯´æ˜ï¼šåœ¨æ’å€¼åã€ç¦»æ•£åŒ–å‰è¿›è¡Œï¼Œä¿è¯å»è¶‹åŠ¿åœ¨è¿ç»­å€¼ä¸Šæ“ä½œ
        if self.detrend:
            print(f"\nã€æ­¥éª¤4/5ã€‘å»è¶‹åŠ¿: ä½¿ç”¨ {self.detrend_method} æ–¹æ³•...")
            df_cleaned = self.detrend_data(df_cleaned)
        else:
            print("\nã€æ­¥éª¤4/5ã€‘å»è¶‹åŠ¿: è·³è¿‡ï¼ˆä¿ç•™åŸå§‹è¶‹åŠ¿ï¼‰")
        
        # æ­¥éª¤5: å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        # è¯´æ˜ï¼šæœ€åä¸€æ­¥ï¼Œå°†è¿ç»­å€¼è½¬æ¢ä¸ºç¦»æ•£ç±»åˆ«
        if self.discretize:
            print(f"\nã€æ­¥éª¤5/5ã€‘ç¦»æ•£åŒ–: å½’ä¸€åŒ–å¹¶ç¦»æ•£åŒ–ä¸º {self.n_bins} ç»„...")
            df_cleaned = self.normalize_and_discretize(df_cleaned)
        else:
            print("\nã€æ­¥éª¤5/5ã€‘ç¦»æ•£åŒ–: è·³è¿‡ï¼ˆä¿æŒè¿ç»­å€¼ï¼‰")

        print("\n" + "="*80)
        print("âœ… æ•°æ®å¤„ç†æµç¨‹å®Œæˆ")
        print("="*80)

        return df_cleaned

    def ensure_complete_time_series(self, df):
        """
        ç¡®ä¿æ¯ä¸ªç«™ç‚¹éƒ½æœ‰å®Œæ•´çš„æ—¶é—´åºåˆ—è®°å½•
        
        é—®é¢˜ï¼šæœ‰äº›ç«™ç‚¹å¯èƒ½è¿ç»­å‡ å¹´éƒ½æ²¡æœ‰è®°å½•ï¼ˆæ•´è¡Œç¼ºå¤±ï¼‰
        è§£å†³ï¼šä¸ºæ¯ä¸ªç«™ç‚¹åˆ›å»ºå®Œæ•´çš„æ—¥æœŸèŒƒå›´ï¼Œç¼ºå¤±çš„æ—¥æœŸç”¨NaNå¡«å……
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•åº”åœ¨æ—¶é—´èšåˆä¹‹åè°ƒç”¨ï¼Œä¼šæ ¹æ®æ—¶é—´èšåˆæ–¹å¼è‡ªåŠ¨è°ƒæ•´é¢‘ç‡
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            DataFrame: åŒ…å«å®Œæ•´æ—¶é—´åºåˆ—çš„æ•°æ®æ¡†
        """
        print("   ç¡®ä¿æ¯ä¸ªç«™ç‚¹éƒ½æœ‰å®Œæ•´çš„æ—¶é—´åºåˆ—...")
        
        # è·å–å…¨å±€æ—¥æœŸèŒƒå›´
        min_date = df['date'].min()
        max_date = df['date'].max()
        print(f"      æ•°æ®æ—¥æœŸèŒƒå›´: {min_date} åˆ° {max_date}")
        
        # æ ¹æ®æ—¶é—´èšåˆæ–¹å¼ç¡®å®šé¢‘ç‡
        freq_map = {
            'daily': 'D',       # æ¯æ—¥
            'monthly': 'MS',    # æ¯æœˆç¬¬ä¸€å¤©
            'quarterly': 'QS',  # æ¯å­£åº¦ç¬¬ä¸€å¤©
            'yearly': 'YS'      # æ¯å¹´ç¬¬ä¸€å¤©
        }
        freq = freq_map.get(self.time_aggregation, 'D')
        
        # åˆ›å»ºå®Œæ•´çš„æ—¥æœŸåºåˆ—
        full_date_range = pd.date_range(start=min_date, end=max_date, freq=freq)
        
        time_unit_map = {
            'daily': 'å¤©',
            'monthly': 'æœˆ',
            'quarterly': 'å­£åº¦',
            'yearly': 'å¹´'
        }
        time_unit = time_unit_map.get(self.time_aggregation, 'å¤©')
        print(f"      å®Œæ•´æ—¶é—´åºåˆ—é•¿åº¦: {len(full_date_range)} {time_unit}")
        
        # è·å–æ‰€æœ‰ç«™ç‚¹åˆ—è¡¨
        all_sites = df['site_id'].unique()
        print(f"      ç«™ç‚¹æ•°é‡: {len(all_sites)}")
        
        # ç»Ÿè®¡åŸå§‹æ•°æ®è¡Œæ•°
        original_rows = len(df)
        
        # ä¸ºæ¯ä¸ªç«™ç‚¹åˆ›å»ºå®Œæ•´çš„æ—¶é—´åºåˆ—
        complete_data_list = []
        
        for site_id in tqdm(all_sites, desc="      åˆ›å»ºå®Œæ•´æ—¶é—´åºåˆ—", leave=False):
            # è·å–è¯¥ç«™ç‚¹çš„å®é™…æ•°æ®
            site_data = df[df['site_id'] == site_id].copy()
            
            # åˆ›å»ºè¯¥ç«™ç‚¹çš„å®Œæ•´æ—¥æœŸæ¡†æ¶
            site_complete = pd.DataFrame({
                'site_id': site_id,
                'date': full_date_range
            })
            
            # å°†å®é™…æ•°æ®åˆå¹¶åˆ°å®Œæ•´æ¡†æ¶ä¸­
            # ä½¿ç”¨å·¦è¿æ¥ï¼Œä¿ç•™æ‰€æœ‰æ—¥æœŸï¼Œç¼ºå¤±çš„ç‰¹å¾å€¼ä¼šæ˜¯NaN
            site_complete = site_complete.merge(
                site_data,
                on=['site_id', 'date'],
                how='left'
            )
            
            complete_data_list.append(site_complete)
        
        # åˆå¹¶æ‰€æœ‰ç«™ç‚¹çš„æ•°æ®
        df_complete = pd.concat(complete_data_list, ignore_index=True)
        
        # æŒ‰ç«™ç‚¹å’Œæ—¥æœŸæ’åº
        df_complete = df_complete.sort_values(['site_id', 'date']).reset_index(drop=True)
        
        # ç»Ÿè®¡ç»“æœ
        new_rows = len(df_complete)
        added_rows = new_rows - original_rows
        
        print(f"      âœ… å®Œæ•´æ—¶é—´åºåˆ—åˆ›å»ºå®Œæˆ")
        print(f"         åŸå§‹æ•°æ®è¡Œæ•°: {original_rows:,}")
        print(f"         è¡¥å…¨åè¡Œæ•°: {new_rows:,}")
        print(f"         æ–°å¢è¡Œæ•°: {added_rows:,} ({added_rows/original_rows*100:.1f}%)")
        print(f"         å¹³å‡æ¯ç«™ç‚¹: {new_rows/len(all_sites):.1f} æ¡è®°å½•")
        
        return df_complete

    def fill_missing_values(self, df):
        """
        å¡«å……ç¼ºå¤±å€¼ï¼ˆæŒ‰ç«™ç‚¹è¿›è¡Œæ—¶é—´åºåˆ—æ’å€¼ï¼‰
        
        ç­–ç•¥ï¼š
        1. å¯¹æ¯ä¸ªç«™ç‚¹çš„è¿ç»­ç‰¹å¾ï¼Œä½¿ç”¨çº¿æ€§æ’å€¼
        2. å¦‚æœå¼€å¤´/ç»“å°¾ä»æœ‰NaNï¼Œä½¿ç”¨å‰å‘/åå‘å¡«å……
        3. å¦‚æœæ•´åˆ—éƒ½æ˜¯NaNï¼Œä½¿ç”¨å…¨å±€å‡å€¼
        4. äºŒè¿›åˆ¶ç‰¹å¾ç”¨0å¡«å……ï¼ˆè¡¨ç¤ºäº‹ä»¶æœªå‘ç”Ÿï¼‰
        
        æ³¨æ„ï¼šæ­¤æ–¹æ³•åº”åœ¨ ensure_complete_time_series ä¹‹åè°ƒç”¨
        """
        print("   å¡«å……ç¼ºå¤±å€¼...")
        
        df_filled = df.copy()
        
        # ç»Ÿè®¡åŸå§‹ç¼ºå¤±æƒ…å†µ
        original_na_counts = {}
        for feature in self.continuous_features:
            if feature in df_filled.columns:
                original_na_counts[feature] = df_filled[feature].isna().sum()
        
        # æŒ‰ç«™ç‚¹åˆ†ç»„å¤„ç†è¿ç»­ç‰¹å¾
        print("      å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œæ—¶é—´åºåˆ—æ’å€¼...")
        
        # é¦–å…ˆè®¡ç®—æ¯ä¸ªç‰¹å¾çš„å…¨å±€ç»Ÿè®¡é‡ï¼ˆç”¨äºé•¿æœŸç¼ºå¤±çš„ç«™ç‚¹ï¼‰
        global_stats = {}
        for feature in self.continuous_features:
            if feature in df_filled.columns:
                global_stats[feature] = {
                    'mean': df_filled[feature].mean(),
                    'median': df_filled[feature].median(),
                    'std': df_filled[feature].std()
                }
        
        for site_id in tqdm(df_filled['site_id'].unique(), desc="      å¤„ç†ç«™ç‚¹", leave=False):
            site_mask = df_filled['site_id'] == site_id
            
            for feature in self.continuous_features:
                if feature not in df_filled.columns:
                    continue
                
                # è·å–è¯¥ç«™ç‚¹è¯¥ç‰¹å¾çš„æ•°æ®
                site_feature_data = df_filled.loc[site_mask, feature].copy()
                
                if site_feature_data.isna().all():
                    # å¦‚æœè¯¥ç«™ç‚¹è¯¥ç‰¹å¾å…¨æ˜¯NaNï¼Œç”¨å…¨å±€å‡å€¼å¡«å……
                    if pd.notna(global_stats[feature]['mean']):
                        df_filled.loc[site_mask, feature] = global_stats[feature]['mean']
                    continue
                
                # æ£€æŸ¥ç¼ºå¤±æ¯”ä¾‹
                missing_ratio = site_feature_data.isna().sum() / len(site_feature_data)
                
                if missing_ratio > 0.8:
                    # å¦‚æœç¼ºå¤±è¶…è¿‡80%ï¼Œä½¿ç”¨å…¨å±€ä¸­ä½æ•°ä½œä¸ºåŸºå‡†ï¼Œå†åŠ ä¸Šç«™ç‚¹åç§»
                    valid_data = site_feature_data.dropna()
                    if len(valid_data) > 0:
                        site_offset = valid_data.mean() - global_stats[feature]['mean']
                        interpolated = site_feature_data.fillna(
                            global_stats[feature]['mean'] + site_offset
                        )
                    else:
                        interpolated = site_feature_data.fillna(global_stats[feature]['mean'])
                else:
                    # ç¼ºå¤±æ¯”ä¾‹è¾ƒå°ï¼Œä½¿ç”¨å¤šç§æ’å€¼æ–¹æ³•
                    
                    # 1. é¦–å…ˆä½¿ç”¨ä¸‰æ¬¡æ ·æ¡æ’å€¼ï¼ˆå¯¹äºå¹³æ»‘çš„æ•°æ®æ•ˆæœæ›´å¥½ï¼‰
                    try:
                        interpolated = site_feature_data.interpolate(
                            method='cubic',
                            limit_direction='both',
                            limit=365  # æœ€å¤šæ’å€¼365å¤©
                        )
                    except:
                        # å¦‚æœä¸‰æ¬¡æ ·æ¡å¤±è´¥ï¼Œé™çº§ä¸ºçº¿æ€§æ’å€¼
                        interpolated = site_feature_data.interpolate(
                            method='linear',
                            limit_direction='both',
                            limit=365
                        )
                    
                    # 2. å¯¹äºä»ç„¶ç¼ºå¤±çš„å€¼ï¼ˆè¶…è¿‡365å¤©çš„é—´éš”ï¼‰ï¼Œä½¿ç”¨æ—¶é—´åŠ æƒçš„å…¨å±€å‡å€¼
                    if interpolated.isna().any():
                        # ä½¿ç”¨ç›¸åŒæ—¥æœŸçš„å…¨å±€å¹³å‡å€¼ï¼ˆè€ƒè™‘å­£èŠ‚æ€§ï¼‰
                        for idx in interpolated[interpolated.isna()].index:
                            date = df_filled.loc[idx, 'date']
                            # è·å–ç›¸åŒæœˆä»½å’Œæ—¥æœŸçš„æ‰€æœ‰ç«™ç‚¹æ•°æ®
                            same_period_mask = (
                                (df_filled['date'].dt.month == date.month) &
                                (df_filled['date'].dt.day == date.day)
                            )
                            same_period_mean = df_filled.loc[same_period_mask, feature].mean()
                            
                            if pd.notna(same_period_mean):
                                interpolated.loc[idx] = same_period_mean
                            else:
                                # å¦‚æœåŒæœŸæ•°æ®ä¹Ÿç¼ºå¤±ï¼Œä½¿ç”¨å…¨å±€å‡å€¼
                                interpolated.loc[idx] = global_stats[feature]['mean']
                    
                    # 3. å‰å‘å¡«å……ï¼ˆå¤„ç†å¼€å¤´çš„NaNï¼‰
                    interpolated = interpolated.ffill()
                    
                    # 4. åå‘å¡«å……ï¼ˆå¤„ç†ç»“å°¾çš„NaNï¼‰
                    interpolated = interpolated.bfill()
                
                # æ›´æ–°æ•°æ®
                df_filled.loc[site_mask, feature] = interpolated
        
        # 4. å¦‚æœä»æœ‰NaNï¼ˆæ•´ä¸ªç«™ç‚¹éƒ½ç¼ºå¤±æˆ–æŸäº›ç‰¹æ®Šæƒ…å†µï¼‰ï¼Œä½¿ç”¨å…¨å±€å‡å€¼
        for feature in self.continuous_features:
            if feature in df_filled.columns:
                remaining_na = df_filled[feature].isna().sum()
                if remaining_na > 0:
                    global_mean = df_filled[feature].mean()
                    if pd.notna(global_mean):
                        df_filled[feature] = df_filled[feature].fillna(global_mean)
                        print(f"      âš ï¸  {feature}: ç”¨å…¨å±€å‡å€¼ {global_mean:.2f} å¡«å…… {remaining_na} ä¸ªå‰©ä½™NaN")
                    else:
                        # å¦‚æœå…¨å±€å‡å€¼éƒ½æ˜¯NaNï¼ˆæ•´åˆ—éƒ½ç¼ºå¤±ï¼‰ï¼Œå¡«å……ä¸º0
                        df_filled[feature] = df_filled[feature].fillna(0)
                        print(f"      âš ï¸  {feature}: å…¨éƒ¨ç¼ºå¤±ï¼Œç”¨0å¡«å……")
        
        # 5. äºŒè¿›åˆ¶ç‰¹å¾ï¼ˆå¤©æ°”äº‹ä»¶ï¼‰ï¼šNaNå¡«å……ä¸º0ï¼ˆè¡¨ç¤ºæœªå‘ç”Ÿï¼‰
        binary_features = ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']
        for feature in binary_features:
            if feature in df_filled.columns:
                na_count = df_filled[feature].isna().sum()
                if na_count > 0:
                    df_filled[feature] = df_filled[feature].fillna(0)
        
        # æ±‡æ€»å¡«å……ç»“æœ
        print(f"\n      ç¼ºå¤±å€¼å¡«å……ç»“æœï¼š")
        for feature in self.continuous_features:
            if feature in df_filled.columns and feature in original_na_counts:
                original_na = original_na_counts[feature]
                remaining_na = df_filled[feature].isna().sum()
                if original_na > 0:
                    print(f"         {feature:25s}: {original_na:6d} â†’ {remaining_na:6d} NaN")
        
        print(f"      âœ… ç¼ºå¤±å€¼å¡«å……å®Œæˆ")
        
        return df_filled

    def remove_trends_adaptive(self, site_data, feature_name):
        """
        è‡ªé€‚åº”å»è¶‹åŠ¿ï¼šå°è¯•å¤šç§æ–¹æ³•ï¼Œé€‰æ‹©æ•ˆæœæœ€å¥½çš„
        
        Args:
            site_data: å•ä¸ªç«™ç‚¹çš„æŸä¸ªç‰¹å¾çš„æ—¶é—´åºåˆ—æ•°æ®ï¼ˆSeriesï¼‰
            feature_name: ç‰¹å¾åç§°
            
        Returns:
            tuple: (å»è¶‹åŠ¿åçš„æ•°æ®, è¶‹åŠ¿ä¿¡æ¯å­—å…¸)
        """
        original_series = site_data.dropna()
        if len(original_series) < 2:
            return site_data, None
        
        # åŸå§‹è¶‹åŠ¿æ£€æµ‹
        x_orig = np.arange(len(original_series))
        try:
            slope_orig, intercept_orig, r_value_orig, p_value_orig, std_err_orig = linregress(x_orig, original_series)
        except:
            return site_data, None
        
        # å¦‚æœæ²¡æœ‰æ˜¾è‘—è¶‹åŠ¿ï¼Œç›´æ¥è¿”å›
        if p_value_orig >= 0.05:
            return site_data, {
                'method': 'None (no significant trend)',
                'original_slope': slope_orig,
                'original_p_value': p_value_orig,
                'slope_reduction': 0.0,
                'final_slope': slope_orig,
                'final_p_value': p_value_orig
            }
        
        best_detrended_series = original_series
        best_slope_reduction = 0
        best_method = "None"
        best_final_slope = slope_orig
        best_final_p_value = p_value_orig
        
        # 1. çº¿æ€§å›å½’å»è¶‹åŠ¿
        try:
            detrended_linear = original_series - (slope_orig * x_orig + intercept_orig)
            slope_linear, _, _, p_linear, _ = linregress(x_orig, detrended_linear)
            slope_reduction_linear = 1 - abs(slope_linear / slope_orig) if slope_orig != 0 else 1
            if slope_reduction_linear > best_slope_reduction:
                best_slope_reduction = slope_reduction_linear
                best_detrended_series = detrended_linear
                best_method = "Linear Regression"
                best_final_slope = slope_linear
                best_final_p_value = p_linear
        except:
            pass
        
        # 2. å¤šé¡¹å¼å»è¶‹åŠ¿ (äºŒæ¬¡)
        if len(original_series) >= 3:
            try:
                poly_coeffs = np.polyfit(x_orig, original_series, 2)
                poly_trend = np.polyval(poly_coeffs, x_orig)
                detrended_poly = original_series - poly_trend
                slope_poly, _, _, p_poly, _ = linregress(x_orig, detrended_poly)
                slope_reduction_poly = 1 - abs(slope_poly / slope_orig) if slope_orig != 0 else 1
                if slope_reduction_poly > best_slope_reduction:
                    best_slope_reduction = slope_reduction_poly
                    best_detrended_series = detrended_poly
                    best_method = "Polynomial (2nd order)"
                    best_final_slope = slope_poly
                    best_final_p_value = p_poly
            except:
                pass
        
        # 3. å·®åˆ†å»è¶‹åŠ¿
        if len(original_series) >= 2:
            try:
                detrended_diff = original_series.diff().dropna()
                if not detrended_diff.empty and len(detrended_diff) > 1:
                    x_diff = np.arange(len(detrended_diff))
                    slope_diff, _, _, p_diff, _ = linregress(x_diff, detrended_diff)
                    slope_reduction_diff = 1 - abs(slope_diff / slope_orig) if slope_orig != 0 else 1
                    if slope_reduction_diff > best_slope_reduction:
                        best_slope_reduction = slope_reduction_diff
                        best_detrended_series = detrended_diff
                        best_method = "Differencing"
                        best_final_slope = slope_diff
                        best_final_p_value = p_diff
            except:
                pass
        
        # 4. é«˜é€šæ»¤æ³¢å»è¶‹åŠ¿ (Butterworth)
        if len(original_series) > 24:  # è‡³å°‘2å¹´çš„æ•°æ®ï¼ˆå‡è®¾å¹´åº¦æ•°æ®ï¼‰
            try:
                fs = 1.0  # é‡‡æ ·é¢‘ç‡ï¼ˆæ¯å¹´ä¸€ä¸ªç‚¹ï¼‰
                cutoff_freq = 1/10.0  # ç§»é™¤10å¹´ä»¥ä¸Šçš„è¶‹åŠ¿
                nyquist = 0.5 * fs
                normal_cutoff = cutoff_freq / nyquist
                
                if 0 < normal_cutoff < 1:
                    b, a = butter(2, normal_cutoff, btype='high', analog=False)
                    detrended_filter = pd.Series(filtfilt(b, a, original_series.values), index=original_series.index)
                    slope_filter, _, _, p_filter, _ = linregress(x_orig, detrended_filter)
                    slope_reduction_filter = 1 - abs(slope_filter / slope_orig) if slope_orig != 0 else 1
                    if slope_reduction_filter > best_slope_reduction:
                        best_slope_reduction = slope_reduction_filter
                        best_detrended_series = detrended_filter
                        best_method = "High-pass Filter (Butterworth)"
                        best_final_slope = slope_filter
                        best_final_p_value = p_filter
            except:
                pass
        
        # å°†å»è¶‹åŠ¿åçš„æ•°æ®é‡æ–°å¯¹é½åˆ°åŸå§‹ç´¢å¼•
        result = site_data.copy()
        result.loc[original_series.index] = best_detrended_series.values
        
        trend_info = {
            'method': best_method,
            'original_slope': slope_orig,
            'original_p_value': p_value_orig,
            'slope_reduction': best_slope_reduction,
            'final_slope': best_final_slope,
            'final_p_value': best_final_p_value
        }
        
        return result, trend_info

    def detrend_data(self, df):
        """
        å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå»è¶‹åŠ¿å¤„ç†ï¼ˆæŒ‰ç«™ç‚¹åˆ†ç»„ï¼‰
        
        Args:
            df: è¾“å…¥æ•°æ®æ¡†
            
        Returns:
            DataFrame: å»è¶‹åŠ¿åçš„æ•°æ®
        """
        print(f"\nå¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå»è¶‹åŠ¿å¤„ç†ï¼ˆæ–¹æ³•: {self.detrend_method}ï¼‰...")
        
        df_detrended = df.copy()
        detrend_info = {}
        all_trend_stats = []  # ç”¨äºæ”¶é›†æ‰€æœ‰è¶‹åŠ¿ç»Ÿè®¡ä¿¡æ¯
        
        # ç¡®ä¿æ•°æ®æŒ‰ç«™ç‚¹å’Œæ—¥æœŸæ’åº
        df_detrended = df_detrended.sort_values(['site_id', 'date']).reset_index(drop=True)
        
        for feature in self.continuous_features:
            if feature not in df_detrended.columns:
                continue
            
            # ä¿å­˜åŸå§‹å€¼
            df_detrended[f'{feature}_before_detrend'] = df_detrended[feature].copy()
            
            removed_count = 0
            
            # æŒ‰ç«™ç‚¹åˆ†ç»„å¤„ç†
            for site_id in tqdm(df_detrended['site_id'].unique(), desc=f"   å»è¶‹åŠ¿ {feature}", leave=False):
                site_mask = df_detrended['site_id'] == site_id
                site_data = df_detrended.loc[site_mask, feature].copy()
                
                # è·³è¿‡å…¨æ˜¯ NaN çš„æ•°æ®
                if site_data.isna().all():
                    continue
                
                # åº”ç”¨ä¸åŒçš„å»è¶‹åŠ¿æ–¹æ³•
                if self.detrend_method == 'adaptive':
                    # ä½¿ç”¨è‡ªé€‚åº”æ–¹æ³•
                    detrended, trend_stat = self.remove_trends_adaptive(site_data, feature)
                    if trend_stat is not None:
                        trend_stat['site_id'] = site_id
                        trend_stat['feature'] = feature
                        all_trend_stats.append(trend_stat)
                
                elif self.detrend_method == 'difference':
                    # ä¸€é˜¶å·®åˆ†: X(t) - X(t-1)
                    detrended = site_data.diff()
                    # ç¬¬ä¸€ä¸ªå€¼ä¼šæ˜¯ NaNï¼Œæˆ‘ä»¬ä¿ç•™åŸå€¼æˆ–åˆ é™¤
                    # è¿™é‡Œé€‰æ‹©åˆ é™¤ç¬¬ä¸€ä¸ªè§‚æµ‹ï¼ˆå› ä¸ºæ²¡æœ‰å‰ä¸€ä¸ªå€¼å¯ä»¥è®¡ç®—å·®åˆ†ï¼‰
                    removed_count += 1
                    
                elif self.detrend_method == 'linear':
                    # çº¿æ€§å»è¶‹åŠ¿ï¼šæ‹Ÿåˆçº¿æ€§è¶‹åŠ¿å¹¶å‡å»
                    valid_mask = site_data.notna()
                    if valid_mask.sum() > 1:
                        x = np.arange(len(site_data))
                        valid_x = x[valid_mask]
                        valid_y = site_data[valid_mask].values
                        
                        # æ‹Ÿåˆçº¿æ€§å›å½’
                        coeffs = np.polyfit(valid_x, valid_y, 1)
                        trend = np.polyval(coeffs, x)
                        
                        # å‡å»è¶‹åŠ¿
                        detrended = site_data - trend
                    else:
                        detrended = site_data
                
                elif self.detrend_method == 'moving_average':
                    # ç§»åŠ¨å¹³å‡å»è¶‹åŠ¿ï¼šå‡å»ç§»åŠ¨å¹³å‡å€¼
                    # çª—å£å¤§å°æ ¹æ®æ—¶é—´èšåˆæ–¹å¼è°ƒæ•´
                    window_size_map = {
                        'daily': 30,      # 30å¤©
                        'monthly': 12,    # 12ä¸ªæœˆ
                        'quarterly': 4,   # 4ä¸ªå­£åº¦
                        'yearly': 5       # 5å¹´
                    }
                    window = window_size_map.get(self.time_aggregation, 30)
                    
                    # è®¡ç®—ç§»åŠ¨å¹³å‡ï¼ˆä¸­å¿ƒå¯¹é½ï¼‰
                    ma = site_data.rolling(window=window, center=True, min_periods=1).mean()
                    detrended = site_data - ma
                
                elif self.detrend_method == 'seasonal':
                    # å­£èŠ‚æ€§å·®åˆ†
                    # å‘¨æœŸæ ¹æ®æ—¶é—´èšåˆæ–¹å¼è°ƒæ•´
                    period_map = {
                        'daily': 365,     # å¹´å‘¨æœŸ
                        'monthly': 12,    # å¹´å‘¨æœŸ
                        'quarterly': 4,   # å¹´å‘¨æœŸ
                        'yearly': 1       # ä¸é€‚ç”¨
                    }
                    period = period_map.get(self.time_aggregation, 12)
                    
                    if period > 1:
                        detrended = site_data.diff(periods=period)
                        removed_count += period
                    else:
                        # å¹´åº¦æ•°æ®ä¸é€‚åˆå­£èŠ‚æ€§å·®åˆ†ï¼Œä½¿ç”¨ä¸€é˜¶å·®åˆ†
                        detrended = site_data.diff()
                        removed_count += 1
                
                # æ›´æ–°æ•°æ®
                df_detrended.loc[site_mask, feature] = detrended
            
            # ç»Ÿè®¡ä¿¡æ¯
            detrend_info[feature] = {
                'method': self.detrend_method,
                'removed_per_site': removed_count / df_detrended['site_id'].nunique() if df_detrended['site_id'].nunique() > 0 else 0
            }
            
            print(f"      {feature:25s}: å®Œæˆ")
        
        # å¦‚æœä½¿ç”¨å·®åˆ†æ–¹æ³•ï¼Œåˆ é™¤äº§ç”Ÿçš„ NaN è¡Œ
        if self.detrend_method in ['difference', 'seasonal']:
            original_len = len(df_detrended)
            
            # åªåˆ é™¤å› å»è¶‹åŠ¿äº§ç”Ÿçš„ NaNï¼ˆæ‰€æœ‰è¿ç»­ç‰¹å¾éƒ½æ˜¯ NaN çš„è¡Œï¼‰
            continuous_cols_in_df = [f for f in self.continuous_features if f in df_detrended.columns]
            if continuous_cols_in_df:
                df_detrended = df_detrended.dropna(subset=continuous_cols_in_df, how='all')
                removed_rows = original_len - len(df_detrended)
                print(f"\n   å› å·®åˆ†äº§ç”Ÿçš„ NaN è¡Œå·²åˆ é™¤: {removed_rows:,} è¡Œ")
        
        # ç”Ÿæˆè¯¦ç»†çš„è¶‹åŠ¿ç»Ÿè®¡æŠ¥å‘Šï¼ˆä»…å¯¹è‡ªé€‚åº”æ–¹æ³•ï¼‰
        if self.detrend_method == 'adaptive' and all_trend_stats:
            self._generate_trend_report(all_trend_stats)
        
        print(f"   âœ… å»è¶‹åŠ¿å¤„ç†å®Œæˆ")
        print(f"   ä¿ç•™æ•°æ®è¡Œæ•°: {len(df_detrended):,}")
        
        return df_detrended
    
    def _generate_trend_report(self, trend_stats):
        """
        ç”Ÿæˆè¯¦ç»†çš„è¶‹åŠ¿ç§»é™¤ç»Ÿè®¡æŠ¥å‘Š
        
        Args:
            trend_stats: è¶‹åŠ¿ç»Ÿè®¡ä¿¡æ¯åˆ—è¡¨
        """
        print("\n" + "="*80)
        print("ğŸ“Š è‡ªé€‚åº”å»è¶‹åŠ¿æ•ˆæœéªŒè¯å’Œç»Ÿè®¡")
        print("="*80)
        
        df_stats = pd.DataFrame(trend_stats)
        
        # 1. æ–¹æ³•ä½¿ç”¨ç»Ÿè®¡
        print("\nã€1ã€‘å»è¶‹åŠ¿æ–¹æ³•ä½¿ç”¨ç»Ÿè®¡:")
        method_counts = df_stats['method'].value_counts()
        for method, count in method_counts.items():
            percentage = count / len(df_stats) * 100
            print(f"   {method:35s}: {count:4d} æ¬¡ ({percentage:5.1f}%)")
        
        # 2. æŒ‰ç‰¹å¾ç»Ÿè®¡
        print("\nã€2ã€‘å„ç‰¹å¾å»è¶‹åŠ¿æ•ˆæœ:")
        print(f"   {'ç‰¹å¾åç§°':<30s} {'å¤„ç†æ¬¡æ•°':>8s} {'å¹³å‡è¶‹åŠ¿å‡å°‘':>12s} {'æ˜¾è‘—è¶‹åŠ¿æ•°':>10s}")
        print("   " + "-"*70)
        
        for feature in df_stats['feature'].unique():
            feature_data = df_stats[df_stats['feature'] == feature]
            
            # åªç»Ÿè®¡æœ‰æ˜¾è‘—è¶‹åŠ¿çš„ï¼ˆp < 0.05ï¼‰
            significant = feature_data[feature_data['original_p_value'] < 0.05]
            
            if len(significant) > 0:
                avg_reduction = significant['slope_reduction'].mean() * 100
                print(f"   {feature:<30s} {len(feature_data):>8d} {avg_reduction:>11.1f}% {len(significant):>10d}")
            else:
                print(f"   {feature:<30s} {len(feature_data):>8d} {'N/A':>12s} {len(significant):>10d}")
        
        # 3. æ•´ä½“æ•ˆæœç»Ÿè®¡
        print("\nã€3ã€‘æ•´ä½“å»è¶‹åŠ¿æ•ˆæœ:")
        significant_trends = df_stats[df_stats['original_p_value'] < 0.05]
        
        if len(significant_trends) > 0:
            print(f"   æ€»å¤„ç†æ¬¡æ•°: {len(df_stats)}")
            print(f"   æ˜¾è‘—è¶‹åŠ¿æ•° (p<0.05): {len(significant_trends)} ({len(significant_trends)/len(df_stats)*100:.1f}%)")
            print(f"   å¹³å‡åŸå§‹æ–œç‡: {significant_trends['original_slope'].abs().mean():.6f}")
            print(f"   å¹³å‡æœ€ç»ˆæ–œç‡: {significant_trends['final_slope'].abs().mean():.6f}")
            print(f"   å¹³å‡è¶‹åŠ¿å‡å°‘ç‡: {significant_trends['slope_reduction'].mean()*100:.1f}%")
            
            # æ•ˆæœåˆ†çº§
            excellent = (significant_trends['slope_reduction'] >= 0.9).sum()
            good = ((significant_trends['slope_reduction'] >= 0.7) & (significant_trends['slope_reduction'] < 0.9)).sum()
            moderate = ((significant_trends['slope_reduction'] >= 0.5) & (significant_trends['slope_reduction'] < 0.7)).sum()
            poor = (significant_trends['slope_reduction'] < 0.5).sum()
            
            print(f"\n   æ•ˆæœåˆ†çº§:")
            print(f"      ä¼˜ç§€ (â‰¥90%å‡å°‘): {excellent} ({excellent/len(significant_trends)*100:.1f}%)")
            print(f"      è‰¯å¥½ (70-90%):   {good} ({good/len(significant_trends)*100:.1f}%)")
            print(f"      ä¸­ç­‰ (50-70%):   {moderate} ({moderate/len(significant_trends)*100:.1f}%)")
            print(f"      è¾ƒå·® (<50%):     {poor} ({poor/len(significant_trends)*100:.1f}%)")
        else:
            print(f"   æœªæ£€æµ‹åˆ°æ˜¾è‘—è¶‹åŠ¿ (æ‰€æœ‰ p-value â‰¥ 0.05)")
        
        # 4. ä¿å­˜è¯¦ç»†æŠ¥å‘Šåˆ°æ–‡ä»¶
        report_path = self.output_dir / "trend_removal_detailed_report.csv"
        df_stats.to_csv(report_path, index=False)
        print(f"\n   âœ… è¯¦ç»†è¶‹åŠ¿ç»Ÿè®¡å·²ä¿å­˜åˆ°: {report_path}")
        
        # 5. ä¿å­˜æ–‡æœ¬æ‘˜è¦
        summary_path = self.output_dir / "trend_removal_summary.txt"
        with open(summary_path, 'w', encoding='utf-8') as f:
            f.write("è‡ªé€‚åº”å»è¶‹åŠ¿æ•ˆæœéªŒè¯å’Œç»Ÿè®¡æŠ¥å‘Š\n")
            f.write("="*80 + "\n\n")
            f.write(f"å»è¶‹åŠ¿æ–¹æ³•: è‡ªé€‚åº”é€‰æ‹©ï¼ˆçº¿æ€§/å¤šé¡¹å¼/å·®åˆ†/é«˜é€šæ»¤æ³¢ï¼‰\n")
            f.write(f"æ˜¾è‘—æ€§æ°´å¹³: p < 0.05\n")
            f.write(f"å¤„ç†æ—¥æœŸ: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n")
            
            f.write("="*80 + "\n")
            f.write("1. æ–¹æ³•ä½¿ç”¨ç»Ÿè®¡\n")
            f.write("="*80 + "\n")
            for method, count in method_counts.items():
                percentage = count / len(df_stats) * 100
                f.write(f"{method:35s}: {count:4d} æ¬¡ ({percentage:5.1f}%)\n")
            
            f.write("\n" + "="*80 + "\n")
            f.write("2. å„ç‰¹å¾å»è¶‹åŠ¿æ•ˆæœ\n")
            f.write("="*80 + "\n")
            for feature in df_stats['feature'].unique():
                feature_data = df_stats[df_stats['feature'] == feature]
                significant = feature_data[feature_data['original_p_value'] < 0.05]
                
                f.write(f"\n{feature}:\n")
                f.write(f"  æ€»å¤„ç†æ¬¡æ•°: {len(feature_data)}\n")
                f.write(f"  æ˜¾è‘—è¶‹åŠ¿æ•°: {len(significant)}\n")
                
                if len(significant) > 0:
                    f.write(f"  å¹³å‡è¶‹åŠ¿å‡å°‘: {significant['slope_reduction'].mean()*100:.1f}%\n")
                    f.write(f"  å¹³å‡åŸå§‹æ–œç‡: {significant['original_slope'].abs().mean():.6f}\n")
                    f.write(f"  å¹³å‡æœ€ç»ˆæ–œç‡: {significant['final_slope'].abs().mean():.6f}\n")
                    
                    # åˆ—å‡ºä½¿ç”¨çš„æ–¹æ³•
                    methods_used = significant['method'].value_counts()
                    f.write(f"  ä½¿ç”¨çš„æ–¹æ³•:\n")
                    for method, count in methods_used.items():
                        f.write(f"    - {method}: {count} æ¬¡\n")
            
            if len(significant_trends) > 0:
                f.write("\n" + "="*80 + "\n")
                f.write("3. æ•´ä½“æ•ˆæœç»Ÿè®¡\n")
                f.write("="*80 + "\n")
                f.write(f"æ€»å¤„ç†æ¬¡æ•°: {len(df_stats)}\n")
                f.write(f"æ˜¾è‘—è¶‹åŠ¿æ•° (p<0.05): {len(significant_trends)} ({len(significant_trends)/len(df_stats)*100:.1f}%)\n")
                f.write(f"å¹³å‡è¶‹åŠ¿å‡å°‘ç‡: {significant_trends['slope_reduction'].mean()*100:.1f}%\n\n")
                f.write(f"æ•ˆæœåˆ†çº§:\n")
                f.write(f"  ä¼˜ç§€ (â‰¥90%): {excellent} ({excellent/len(significant_trends)*100:.1f}%)\n")
                f.write(f"  è‰¯å¥½ (70-90%): {good} ({good/len(significant_trends)*100:.1f}%)\n")
                f.write(f"  ä¸­ç­‰ (50-70%): {moderate} ({moderate/len(significant_trends)*100:.1f}%)\n")
                f.write(f"  è¾ƒå·® (<50%): {poor} ({poor/len(significant_trends)*100:.1f}%)\n")
        
        print(f"   âœ… è¶‹åŠ¿æ‘˜è¦å·²ä¿å­˜åˆ°: {summary_path}")
        print("="*80)

    def normalize_and_discretize(self, df):
        """
        å¯¹è¿ç»­ç‰¹å¾å’ŒäºŒå…ƒç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–
        """
        print("\nå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ç‰¹å¾...")
        print(f"   ç¦»æ•£åŒ–ç»„æ•°: {self.n_bins}")

        df_processed = df.copy()
        normalization_info = {}

        # å¤„ç†è¿ç»­ç‰¹å¾
        print("\n   å¤„ç†è¿ç»­ç‰¹å¾:")
        for feature in self.continuous_features:
            if feature not in df_processed.columns:
                continue

            valid_mask = df_processed[feature].notna()
            valid_data = df_processed.loc[valid_mask, feature]

            if len(valid_data) == 0:
                print(f"      {feature}: å…¨éƒ¨ç¼ºå¤±ï¼Œè·³è¿‡")
                continue

            min_val = valid_data.min()
            max_val = valid_data.max()

            if max_val > min_val:
                normalized = (valid_data - min_val) / (max_val - min_val)
                discretized = pd.cut(
                    normalized,
                    bins=self.n_bins,
                    labels=range(self.n_bins),
                    include_lowest=True
                )
                discretized = discretized.astype(float)

                df_processed[f'{feature}_raw'] = df_processed[feature]
                df_processed.loc[valid_mask, feature] = discretized

                normalization_info[feature] = {
                    'type': 'continuous',
                    'min': min_val,
                    'max': max_val,
                    'bins': self.n_bins,
                    'unique_values': len(valid_data.unique())
                }

                print(f"      {feature:25s}: [{min_val:.2f}, {max_val:.2f}] -> [0, {self.n_bins-1}]")
            else:
                print(f"      {feature}: å€¼èŒƒå›´ä¸º 0ï¼Œè·³è¿‡ç¦»æ•£åŒ–")

        # å¤„ç†äºŒå…ƒç‰¹å¾
        print("\n   å¤„ç†äºŒå…ƒç‰¹å¾:")
        for feature in self.binary_features:
            if feature not in df_processed.columns:
                continue

            valid_mask = df_processed[feature].notna()
            valid_data = df_processed.loc[valid_mask, feature]

            if len(valid_data) == 0:
                print(f"      {feature}: å…¨éƒ¨ç¼ºå¤±ï¼Œè·³è¿‡")
                continue

            min_val = valid_data.min()
            max_val = valid_data.max()

            # å½’ä¸€åŒ–åˆ° [0, 1]
            if max_val > min_val:
                normalized = (valid_data - min_val) / (max_val - min_val)
            else:
                # å¦‚æœæ‰€æœ‰å€¼ç›¸åŒï¼Œä¿æŒä¸º0
                normalized = pd.Series(0, index=valid_data.index)

            # ç¦»æ•£åŒ–ä¸º n_bins ç»„
            discretized = pd.cut(
                normalized,
                bins=self.n_bins,
                labels=range(self.n_bins),
                include_lowest=True
            )
            discretized = discretized.astype(float)

            df_processed[f'{feature}_raw'] = df_processed[feature]
            df_processed.loc[valid_mask, feature] = discretized

            normalization_info[feature] = {
                'type': 'binary',
                'min': min_val,
                'max': max_val,
                'bins': self.n_bins,
                'unique_values': len(valid_data.unique())
            }

            print(f"      {feature:25s}: [{min_val:.2f}, {max_val:.2f}] -> [0, {self.n_bins-1}]")

        info_path = self.output_dir / "normalization_info.txt"
        with open(info_path, 'w') as f:
            f.write("ç‰¹å¾å½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ä¿¡æ¯\n")
            f.write("=" * 80 + "\n\n")
            f.write(f"ç¦»æ•£åŒ–ç»„æ•°: {self.n_bins}\n")
            f.write("ç¦»æ•£åŒ–æ–¹æ³•: ç­‰å®½åˆ†ç®±\n")
            f.write(f"ç»„æ ‡ç­¾: 0, 1, ..., {self.n_bins-1}\n\n")

            f.write("è¿ç»­ç‰¹å¾:\n")
            f.write("-" * 40 + "\n")
            for feature, info in normalization_info.items():
                if info['type'] == 'continuous':
                    f.write(f"\n{feature}:\n")
                    f.write(f"  åŸå§‹èŒƒå›´: [{info['min']:.2f}, {info['max']:.2f}]\n")
                    f.write(f"  å”¯ä¸€å€¼æ•°: {info['unique_values']}\n")
                    f.write(f"  ç¦»æ•£åŒ–å: {info['bins']} ç»„ (0-{info['bins']-1})\n")

            f.write("\näºŒå…ƒç‰¹å¾:\n")
            f.write("-" * 40 + "\n")
            for feature, info in normalization_info.items():
                if info['type'] == 'binary':
                    f.write(f"\n{feature}:\n")
                    f.write(f"  åŸå§‹èŒƒå›´: [{info['min']:.2f}, {info['max']:.2f}]\n")
                    f.write(f"  å”¯ä¸€å€¼æ•°: {info['unique_values']}\n")
                    f.write(f"  ç¦»æ•£åŒ–å: {info['bins']} ç»„ (0-{info['bins']-1})\n")

        print(f"\n   å½’ä¸€åŒ–ä¿¡æ¯å·²ä¿å­˜åˆ°: {info_path}")
        continuous_count = sum(1 for info in normalization_info.values() if info['type'] == 'continuous')
        binary_count = sum(1 for info in normalization_info.values() if info['type'] == 'binary')
        print(f"   ç¦»æ•£åŒ–å®Œæˆ: {continuous_count} ä¸ªè¿ç»­ç‰¹å¾, {binary_count} ä¸ªäºŒå…ƒç‰¹å¾")

        return df_processed

    def aggregate_to_quarterly(self, df):
        """
        å°†æ¯æ—¥æ•°æ®èšåˆä¸ºå­£åº¦å¹³å‡æ•°æ®
        
        å­£åº¦åˆ’åˆ†ï¼š
        - Q1: 1-3æœˆï¼ˆå†¬æ˜¥ï¼‰
        - Q2: 4-6æœˆï¼ˆæ˜¥å¤ï¼‰
        - Q3: 7-9æœˆï¼ˆå¤ç§‹ï¼‰
        - Q4: 10-12æœˆï¼ˆç§‹å†¬ï¼‰
        
        Args:
            df: åŒ…å«æ¯æ—¥è§‚æµ‹çš„ DataFrame
            
        Returns:
            DataFrame: å­£åº¦å¹³å‡æ•°æ®
        """
        print("   èšåˆä¸ºå­£åº¦å¹³å‡æ•°æ®...")
        
        # æå–å¹´ä»½å’Œå­£åº¦ä¿¡æ¯
        df['year'] = df['date'].dt.year
        df['quarter'] = df['date'].dt.quarter  # Pandas è‡ªåŠ¨è®¡ç®—å­£åº¦ (1-4)
        
        # å®šä¹‰èšåˆè§„åˆ™
        agg_dict = {}
        
        # è¿ç»­ç‰¹å¾ï¼šè®¡ç®—å¹³å‡å€¼
        for feature in self.continuous_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'
                # å¦‚æœæœ‰åŸå§‹å€¼åˆ—ï¼Œä¹Ÿè®¡ç®—å¹³å‡
                if f'{feature}_raw' in df.columns:
                    agg_dict[f'{feature}_raw'] = 'mean'
        
        # äºŒè¿›åˆ¶ç‰¹å¾ï¼ˆå¤©æ°”äº‹ä»¶ï¼‰ï¼šè®¡ç®—å‘ç”Ÿå¤©æ•°å æ¯”
        binary_features = ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']
        for feature in binary_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'
        
        # æ—¥æœŸï¼šå–è¯¥å­£åº¦çš„ç¬¬ä¸€å¤©ä½œä¸ºä»£è¡¨
        agg_dict['date'] = 'first'
        
        # æŒ‰ç«™ç‚¹ã€å¹´ã€å­£åº¦åˆ†ç»„èšåˆ
        print(f"      æŒ‰ç«™ç‚¹å’Œå­£åº¦åˆ†ç»„èšåˆ...")
        df_quarterly = df.groupby(['site_id', 'year', 'quarter'], as_index=False).agg(agg_dict)
        
        # å°†æ—¥æœŸè®¾ç½®ä¸ºæ¯å­£åº¦ç¬¬ä¸€ä¸ªæœˆçš„1å·
        # Q1: 1æœˆ1æ—¥, Q2: 4æœˆ1æ—¥, Q3: 7æœˆ1æ—¥, Q4: 10æœˆ1æ—¥
        quarter_to_month = {1: '01', 2: '04', 3: '07', 4: '10'}
        df_quarterly['date'] = pd.to_datetime(
            df_quarterly['year'].astype(str) + '-' + 
            df_quarterly['quarter'].map(quarter_to_month) + '-01'
        )
        
        # åˆ é™¤ä¸´æ—¶çš„ year å’Œ quarter åˆ—
        df_quarterly = df_quarterly.drop(columns=['year', 'quarter'])
        
        # é‡æ–°æ’åº
        df_quarterly = df_quarterly.sort_values(['site_id', 'date']).reset_index(drop=True)
        
        print(f"      èšåˆå®Œæˆ:")
        print(f"         åŸå§‹æ•°æ®: {len(df):,} è¡Œ")
        print(f"         å­£åº¦æ•°æ®: {len(df_quarterly):,} è¡Œ")
        print(f"         å¹³å‡æ¯ç«™ç‚¹: {len(df_quarterly) / df_quarterly['site_id'].nunique():.1f} ä¸ªå­£åº¦")
        
        return df_quarterly

    def aggregate_to_monthly(self, df):
        """
        å°†æ¯æ—¥æ•°æ®èšåˆä¸ºæœˆå¹³å‡æ•°æ®
        
        Args:
            df: åŒ…å«æ¯æ—¥è§‚æµ‹çš„ DataFrame
            
        Returns:
            DataFrame: æœˆå¹³å‡æ•°æ®
        """
        print("   èšåˆä¸ºæœˆå¹³å‡æ•°æ®...")
        
        # æå–å¹´-æœˆä¿¡æ¯
        df['year'] = df['date'].dt.year
        df['month'] = df['date'].dt.month
        
        # å®šä¹‰èšåˆè§„åˆ™
        agg_dict = {}
        
        # è¿ç»­ç‰¹å¾ï¼šè®¡ç®—å¹³å‡å€¼
        for feature in self.continuous_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'
                # å¦‚æœæœ‰åŸå§‹å€¼åˆ—ï¼Œä¹Ÿè®¡ç®—å¹³å‡
                if f'{feature}_raw' in df.columns:
                    agg_dict[f'{feature}_raw'] = 'mean'
        
        # äºŒè¿›åˆ¶ç‰¹å¾ï¼ˆå¤©æ°”äº‹ä»¶ï¼‰ï¼šè®¡ç®—å‘ç”Ÿå¤©æ•°å æ¯”æˆ–æ€»æ¬¡æ•°
        binary_features = ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']
        for feature in binary_features:
            if feature in df.columns:
                # è®¡ç®—è¯¥æœˆè¯¥äº‹ä»¶å‘ç”Ÿçš„å¤©æ•°å æ¯”ï¼ˆ0-1ä¹‹é—´ï¼‰
                agg_dict[feature] = 'mean'
        
        # æ—¥æœŸï¼šå–è¯¥æœˆçš„ç¬¬ä¸€å¤©ä½œä¸ºä»£è¡¨
        agg_dict['date'] = 'first'
        
        # æŒ‰ç«™ç‚¹ã€å¹´ã€æœˆåˆ†ç»„èšåˆ
        print(f"      æŒ‰ç«™ç‚¹å’Œæœˆä»½åˆ†ç»„èšåˆ...")
        df_monthly = df.groupby(['site_id', 'year', 'month'], as_index=False).agg(agg_dict)
        
        # å°†æ—¥æœŸè®¾ç½®ä¸ºæ¯æœˆ1å·
        df_monthly['date'] = pd.to_datetime(
            df_monthly['year'].astype(str) + '-' + 
            df_monthly['month'].astype(str).str.zfill(2) + '-01'
        )
        
        # åˆ é™¤ä¸´æ—¶çš„ year å’Œ month åˆ—
        df_monthly = df_monthly.drop(columns=['year', 'month'])
        
        # é‡æ–°æ’åº
        df_monthly = df_monthly.sort_values(['site_id', 'date']).reset_index(drop=True)
        
        print(f"      èšåˆå®Œæˆ:")
        print(f"         åŸå§‹æ•°æ®: {len(df):,} è¡Œ")
        print(f"         æœˆåº¦æ•°æ®: {len(df_monthly):,} è¡Œ")
        print(f"         å¹³å‡æ¯ç«™ç‚¹: {len(df_monthly) / df_monthly['site_id'].nunique():.1f} ä¸ªæœˆ")
        
        return df_monthly

    def aggregate_to_yearly(self, df):
        """å°†æ¯æ—¥æ•°æ®èšåˆä¸ºå¹´å¹³å‡æ•°æ®"""
        print("   èšåˆä¸ºå¹´å¹³å‡æ•°æ®...")

        df['year'] = df['date'].dt.year

        agg_dict = {}
        for feature in self.continuous_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'
                if f'{feature}_raw' in df.columns:
                    agg_dict[f'{feature}_raw'] = 'mean'

        binary_features = ['fog', 'rain', 'snow', 'hail', 'thunder', 'tornado']
        for feature in binary_features:
            if feature in df.columns:
                agg_dict[feature] = 'mean'

        agg_dict['date'] = 'first'

        print("      æŒ‰ç«™ç‚¹å’Œå¹´ä»½åˆ†ç»„èšåˆ...")
        df_yearly = df.groupby(['site_id', 'year'], as_index=False).agg(agg_dict)

        df_yearly['date'] = pd.to_datetime(df_yearly['year'].astype(str) + '-01-01')

        df_yearly = df_yearly.drop(columns=['year'])
        df_yearly = df_yearly.sort_values(['site_id', 'date']).reset_index(drop=True)

        print(f"      èšåˆå®Œæˆ:")
        print(f"         åŸå§‹æ•°æ®: {len(df):,} è¡Œ")
        print(f"         å¹´åº¦æ•°æ®: {len(df_yearly):,} è¡Œ")
        print(f"         å¹³å‡æ¯ç«™ç‚¹: {len(df_yearly) / df_yearly['site_id'].nunique():.1f} å¹´")

        return df_yearly

    def save_processed_data(self, df, filename='processed_weather_data.csv'):
        output_path = self.output_dir / filename
        df.to_csv(output_path, index=False)
        
        # æ—¶é—´ç²’åº¦æ˜¾ç¤º
        time_granularity_map = {
            'daily': 'æ¯æ—¥è§‚æµ‹',
            'monthly': 'æœˆå¹³å‡',
            'quarterly': 'å­£åº¦å¹³å‡',
            'yearly': 'å¹´å¹³å‡'
        }
        
        print(f"\nâœ… æ•°æ®å·²ä¿å­˜åˆ°: {output_path}")
        print(f"   è¡Œæ•°: {len(df):,}")
        print(f"   åˆ—æ•°: {len(df.columns)}")
        print(f"   ç«™ç‚¹æ•°: {df['site_id'].nunique()}")
        print(f"   æ—¥æœŸèŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")
        print(f"   æ—¶é—´ç²’åº¦: {time_granularity_map[self.time_aggregation]}")
        print(f"   å®Œæ•´æ—¶é—´åºåˆ—: {'æ˜¯' if self.complete_time_series else 'å¦'}")
        
        if self.detrend:
            detrend_method_map = {
                'adaptive': 'è‡ªé€‚åº”æ–¹æ³•ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä½³ï¼‰',
                'difference': 'ä¸€é˜¶å·®åˆ†',
                'linear': 'çº¿æ€§å»è¶‹åŠ¿',
                'moving_average': 'ç§»åŠ¨å¹³å‡å»è¶‹åŠ¿',
                'seasonal': 'å­£èŠ‚æ€§å·®åˆ†'
            }
            print(f"   å»è¶‹åŠ¿æ–¹æ³•: {detrend_method_map[self.detrend_method]}")
            before_detrend_cols = [c for c in df.columns if c.endswith('_before_detrend')]
            print(f"   å»è¶‹åŠ¿å‰çš„åŸå§‹å€¼åˆ—ï¼ˆ*_before_detrendï¼‰: {len(before_detrend_cols)} ä¸ª")

        if self.discretize:
            raw_cols = [c for c in df.columns if c.endswith('_raw')]
            print(f"   ç¦»æ•£åŒ–ç‰¹å¾æ•°: {len(self.continuous_features)}")
            print(f"   ç¦»æ•£åŒ–ç»„æ•°: {self.n_bins}")
            print(f"   åŸå§‹å€¼åˆ—ï¼ˆ*_rawï¼‰: {len(raw_cols)} ä¸ª")

    def process_multiple_years(self, years, max_stations_per_year=None):
        all_years_data = []

        for year in years:
            df = self.process_year_data(year, max_stations=max_stations_per_year)
            if df is not None:
                all_years_data.append(df)

        if not all_years_data:
            print("æ²¡æœ‰æˆåŠŸå¤„ç†ä»»ä½•å¹´ä»½çš„æ•°æ®")
            return None

        print(f"\nåˆå¹¶ {len(all_years_data)} ä¸ªå¹´ä»½çš„æ•°æ®...")
        combined_df = pd.concat(all_years_data, ignore_index=True)
        cleaned_df = self.clean_and_transform(combined_df)
        return cleaned_df

    def generate_summary_statistics(self, df):
        print("\næ•°æ®æ‘˜è¦ç»Ÿè®¡:")
        print("=" * 60)

        print(f"\nåŸºæœ¬ä¿¡æ¯:")
        print(f"  æ€»è¡Œæ•°: {len(df):,}")
        print(f"  æ€»ç«™ç‚¹æ•°: {df['site_id'].nunique():,}")
        print(f"  æ—¥æœŸèŒƒå›´: {df['date'].min()} åˆ° {df['date'].max()}")

        obs_per_site = df.groupby('site_id').size()
        print(f"\næ¯ä¸ªç«™ç‚¹çš„è§‚æµ‹å¤©æ•°:")
        print(f"  å¹³å‡: {obs_per_site.mean():.1f} å¤©")
        print(f"  ä¸­ä½æ•°: {obs_per_site.median():.1f} å¤©")
        print(f"  æœ€å°: {obs_per_site.min()} å¤©")
        print(f"  æœ€å¤§: {obs_per_site.max()} å¤©")

        print(f"\nç‰¹å¾ç¼ºå¤±ç‡:")
        numeric_cols = df.select_dtypes(include=[np.number]).columns
        for col in numeric_cols:
            missing_rate = df[col].isna().sum() / len(df) * 100
            print(f"  {col}: {missing_rate:.2f}%")

        print("=" * 60)


def main():
    print("=" * 80)
    print("ğŸŒ¤ï¸  NOAA GSOD æ•°æ®å¤„ç†å·¥å…·")
    print("=" * 80)

    # 1. æ—¶é—´ç²’åº¦é€‰æ‹©
    print("\nã€1/4ã€‘é€‰æ‹©æ—¶é—´ç²’åº¦ï¼š")
    print("1. æ¯æ—¥è§‚æµ‹ï¼ˆDailyï¼‰- ä¿æŒåŸå§‹æ¯æ—¥æ•°æ®")
    print("2. æœˆå¹³å‡ï¼ˆMonthlyï¼‰- å°†æ¯æ—¥æ•°æ®èšåˆä¸ºæœˆå¹³å‡ï¼ˆæ•°æ®é‡çº¦å‡å°‘ 1/30ï¼‰")
    print("3. å­£åº¦å¹³å‡ï¼ˆQuarterlyï¼‰- å°†æ¯æ—¥æ•°æ®èšåˆä¸ºå­£åº¦å¹³å‡ï¼ˆæ•°æ®é‡çº¦å‡å°‘ 1/90ï¼‰")
    print("4. å¹´å¹³å‡ï¼ˆYearlyï¼‰- å°†æ¯æ—¥æ•°æ®èšåˆä¸ºå¹´å¹³å‡ï¼ˆæ•°æ®é‡çº¦å‡å°‘ 1/365ï¼‰")
    
    time_choice = input("\nè¯·é€‰æ‹© (1/2/3/4) [é»˜è®¤: 1]: ").strip() or "1"
    
    time_aggregation_map = {
        "1": "daily",
        "2": "monthly",
        "3": "quarterly",
        "4": "yearly"
    }
    time_aggregation = time_aggregation_map.get(time_choice, "daily")
    
    time_display = {
        "daily": "æ¯æ—¥è§‚æµ‹",
        "monthly": "æœˆå¹³å‡èšåˆ",
        "quarterly": "å­£åº¦å¹³å‡èšåˆ",
        "yearly": "å¹´å¹³å‡èšåˆ"
    }
    print(f"âœ“ å·²é€‰æ‹©: {time_display[time_aggregation]}")

    # 2. å»è¶‹åŠ¿é€‰æ‹©
    print("\nã€2/5ã€‘æ˜¯å¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå»è¶‹åŠ¿ï¼ˆDetrendï¼‰å¤„ç†ï¼Ÿ")
    print("è¯´æ˜ï¼šå»è¶‹åŠ¿å¯ä»¥ç§»é™¤æ•°æ®ä¸­çš„é•¿æœŸè¶‹åŠ¿ï¼Œä½¿æ•°æ®æ›´åŠ å¹³ç¨³ï¼ˆstationaryï¼‰")
    print("1. å¦ - ä¿ç•™åŸå§‹è¶‹åŠ¿ï¼ˆé»˜è®¤ï¼‰")
    print("2. æ˜¯ - ç§»é™¤è¶‹åŠ¿")
    
    detrend_choice = input("\nè¯·é€‰æ‹© (1/2) [é»˜è®¤: 1]: ").strip() or "1"
    detrend = (detrend_choice == "2")
    
    detrend_method = 'adaptive'  # é»˜è®¤æ–¹æ³•æ”¹ä¸ºè‡ªé€‚åº”
    if detrend:
        print("\n   é€‰æ‹©å»è¶‹åŠ¿æ–¹æ³•ï¼š")
        print("   1. è‡ªé€‚åº”æ–¹æ³•ï¼ˆAdaptiveï¼‰- ğŸŒŸæ¨èï¼šè‡ªåŠ¨é€‰æ‹©æœ€ä½³æ–¹æ³•ï¼ˆçº¿æ€§/å¤šé¡¹å¼/å·®åˆ†/é«˜é€šæ»¤æ³¢ï¼‰")
        print("   2. ä¸€é˜¶å·®åˆ†ï¼ˆFirst Differenceï¼‰- è®¡ç®—ç›¸é‚»å€¼å·®å¼‚")
        print("   3. çº¿æ€§å»è¶‹åŠ¿ï¼ˆLinear Detrendï¼‰- ç§»é™¤çº¿æ€§è¶‹åŠ¿")
        print("   4. ç§»åŠ¨å¹³å‡å»è¶‹åŠ¿ï¼ˆMoving Averageï¼‰- å‡å»ç§»åŠ¨å¹³å‡å€¼")
        print("   5. å­£èŠ‚æ€§å·®åˆ†ï¼ˆSeasonal Differenceï¼‰- é€‚åˆæœ‰æ˜æ˜¾å­£èŠ‚æ€§çš„æ•°æ®")
        
        method_choice = input("\n   è¯·é€‰æ‹©å»è¶‹åŠ¿æ–¹æ³• (1/2/3/4/5) [é»˜è®¤: 1-è‡ªé€‚åº”]: ").strip() or "1"
        method_map = {
            "1": "adaptive",
            "2": "difference",
            "3": "linear",
            "4": "moving_average",
            "5": "seasonal"
        }
        detrend_method = method_map.get(method_choice, "adaptive")
        
        method_display = {
            "adaptive": "è‡ªé€‚åº”æ–¹æ³•ï¼ˆè‡ªåŠ¨é€‰æ‹©æœ€ä½³ï¼‰",
            "difference": "ä¸€é˜¶å·®åˆ†",
            "linear": "çº¿æ€§å»è¶‹åŠ¿",
            "moving_average": "ç§»åŠ¨å¹³å‡å»è¶‹åŠ¿",
            "seasonal": "å­£èŠ‚æ€§å·®åˆ†"
        }
        print(f"   âœ“ å°†ä½¿ç”¨ {method_display[detrend_method]} æ–¹æ³•")
    else:
        print("   âœ“ ä¿ç•™åŸå§‹è¶‹åŠ¿")

    # 3. ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥
    print("\nã€3/5ã€‘ç¼ºå¤±å€¼å¤„ç†ç­–ç•¥ï¼š")
    print("è¯´æ˜ï¼šéƒ¨åˆ†ç«™ç‚¹å¯èƒ½è¿ç»­å‡ å¹´éƒ½æ²¡æœ‰æ•°æ®è®°å½•")
    print("1. å®Œæ•´å¡«å……ï¼ˆæ¨èï¼‰- ä¸ºæ¯ä¸ªç«™ç‚¹åˆ›å»ºå®Œæ•´æ—¶é—´åºåˆ—ï¼Œä½¿ç”¨æ™ºèƒ½æ’å€¼å¡«è¡¥æ‰€æœ‰ç¼ºå¤±")
    print("2. ä»…å¡«å……å·²æœ‰è®°å½• - åªå¯¹å·²æœ‰è®°å½•ä¸­çš„ç¼ºå¤±å€¼è¿›è¡Œå¡«å……ï¼Œä¸è¡¥å…¨ç¼ºå¤±çš„æ—¥æœŸ")
    
    complete_choice = input("\nè¯·é€‰æ‹© (1/2) [é»˜è®¤: 1]: ").strip() or "1"
    complete_time_series = (complete_choice == "1")
    
    if complete_time_series:
        print("   âœ“ å°†åˆ›å»ºå®Œæ•´æ—¶é—´åºåˆ—å¹¶æ™ºèƒ½å¡«å……æ‰€æœ‰ç¼ºå¤±å€¼")
    else:
        print("   âœ“ ä»…å¡«å……å·²æœ‰è®°å½•ä¸­çš„ç¼ºå¤±å€¼")

    # 4. ç¦»æ•£åŒ–é€‰æ‹©
    print("\nã€4/5ã€‘æ˜¯å¦å¯¹è¿ç»­ç‰¹å¾è¿›è¡Œå½’ä¸€åŒ–å’Œç¦»æ•£åŒ–ï¼Ÿ")
    print("1. æ˜¯ - å½’ä¸€åŒ–åˆ° [0,1] å¹¶ç¦»æ•£åŒ–ä¸º N ç»„ï¼ˆæ¨èç”¨äº HMMï¼‰")
    print("2. å¦ - ä¿æŒåŸå§‹è¿ç»­å€¼")

    discretize_choice = input("\nè¯·é€‰æ‹© (1/2) [é»˜è®¤: 1]: ").strip() or "1"
    discretize = (discretize_choice == "1")

    n_bins = 5
    if discretize:
        n_bins_input = input(f"   ç¦»æ•£åŒ–ç»„æ•° (3-10) [é»˜è®¤: 5]: ").strip()
        if n_bins_input.isdigit():
            n_bins = int(n_bins_input)
            n_bins = max(3, min(10, n_bins))
        print(f"   âœ“ å°†ä½¿ç”¨ {n_bins} ç»„è¿›è¡Œç¦»æ•£åŒ–")
    else:
        print("   âœ“ ä¿æŒè¿ç»­å€¼")

    # 5. ç«™ç‚¹é€‰æ‹©
    print("\nã€5/5ã€‘é€‰æ‹©è¦å¤„ç†çš„ç«™ç‚¹ï¼š")
    station_csv_input = input(
        "   ç«™ç‚¹åˆ—è¡¨ CSV è·¯å¾„ï¼ˆåŒ…å« USAF, WBANï¼›ç•™ç©ºä½¿ç”¨é»˜è®¤ç«™ç‚¹åˆ—è¡¨ï¼‰: "
    ).strip() or None
    
    if station_csv_input:
        print(f"   âœ“ å°†ä½¿ç”¨è‡ªå®šä¹‰ç«™ç‚¹åˆ—è¡¨: {station_csv_input}")
    else:
        print("   âœ“ ä½¿ç”¨é»˜è®¤ç«™ç‚¹åˆ—è¡¨")

    loader = GSODDataLoader(
        n_bins=n_bins,
        discretize=discretize,
        station_list_csv=station_csv_input,
        time_aggregation=time_aggregation,
        detrend=detrend,
        detrend_method=detrend_method,
        complete_time_series=complete_time_series
    )

    # åŠ è½½ç«™ç‚¹å…ƒæ•°æ®ï¼ˆè¿™é‡Œä¼šç”Ÿæˆ target_site_idsï¼‰
    print("\n" + "=" * 80)
    loader.load_station_metadata()

    print("\næ£€æµ‹å¯ç”¨å¹´ä»½...")
    available_years = []
    for tar_file in sorted(loader.gsod_dir.glob("gsod_*.tar")):
        year = int(tar_file.stem.split('_')[1])
        available_years.append(year)

    print(f"   æ‰¾åˆ° {len(available_years)} ä¸ªå¹´ä»½: {available_years[0]} - {available_years[-1]}")

    # 6. å¹´ä»½èŒƒå›´é€‰æ‹©
    print("\nã€6/6ã€‘é€‰æ‹©å¤„ç†å“ªäº›å¹´ä»½çš„æ•°æ®ï¼š")
    print("1. å¿«é€Ÿæµ‹è¯•ï¼ˆ2015å¹´ï¼Œå‰50ä¸ªç›®æ ‡ç«™ç‚¹æ–‡ä»¶ï¼‰")
    print("2. å•å¹´å®Œæ•´ï¼ˆ2015å¹´ï¼Œæ‰€æœ‰ç›®æ ‡ç«™ç‚¹æ–‡ä»¶ï¼‰")
    print("3. è¿‘æœŸæ•°æ®ï¼ˆ1973-2019 å¹´ï¼Œæ‰€æœ‰ç›®æ ‡ç«™ç‚¹ï¼‰")
    print("4. å…¨éƒ¨æ•°æ®ï¼ˆ1901-2019 å¹´ï¼Œæ‰€æœ‰ç›®æ ‡ç«™ç‚¹ï¼‰")

    choice = input(f"\nè¯·é€‰æ‹© (1/2/3/4) [é»˜è®¤: 3]: ").strip() or "3"
    
    print("\n" + "=" * 80)
    print("ğŸš€ å¼€å§‹å¤„ç†æ•°æ®...")
    print("=" * 80)

    # æ ¹æ®é€‰é¡¹ç”Ÿæˆæ–‡ä»¶ååç¼€
    time_suffix = time_aggregation  # 'daily', 'monthly', 'quarterly', 'yearly'
    detrend_suffix = f"detrend_{detrend_method}" if detrend else "raw"
    discrete_suffix = f"bins{n_bins}" if discretize else "continuous"
    
    if choice == "1":
        print("\nğŸ“ å¿«é€Ÿæµ‹è¯•æ¨¡å¼...")
        df = loader.process_year_data(2015, max_stations=50)
        if df is not None:
            cleaned_df = loader.clean_and_transform(df)
            filename = f'weather_2015_sample_{time_suffix}_{detrend_suffix}_{discrete_suffix}.csv'
            loader.save_processed_data(cleaned_df, filename)
            loader.generate_summary_statistics(cleaned_df)

    elif choice == "2":
        print("\nğŸ“ å¤„ç† 2015 å¹´å®Œæ•´æ•°æ®ï¼ˆä»…ç›®æ ‡ç«™ç‚¹ï¼‰...")
        df = loader.process_year_data(2015)
        if df is not None:
            cleaned_df = loader.clean_and_transform(df)
            filename = f'weather_2015_full_{time_suffix}_{detrend_suffix}_{discrete_suffix}.csv'
            loader.save_processed_data(cleaned_df, filename)
            loader.generate_summary_statistics(cleaned_df)

    elif choice == "3":
        print("\nğŸ“ å¤„ç†è¿‘æœŸæ•°æ®ï¼ˆ1973-2019ï¼Œç›®æ ‡ç«™ç‚¹ï¼‰...")
        years = [y for y in available_years if y >= 1973]
        print(f"   å°†å¤„ç† {len(years)} ä¸ªå¹´ä»½")
        cleaned_df = loader.process_multiple_years(years)
        if cleaned_df is not None:
            filename = f'weather_1973_2019_{time_suffix}_{detrend_suffix}_{discrete_suffix}.csv'
            loader.save_processed_data(cleaned_df, filename)
            loader.generate_summary_statistics(cleaned_df)

    elif choice == "4":
        print("\nğŸ“ å¤„ç†å…¨éƒ¨å†å²æ•°æ®ï¼ˆ1901-2019ï¼Œç›®æ ‡ç«™ç‚¹ï¼‰...")
        print(f"   âš ï¸  è­¦å‘Šï¼šå…¨éƒ¨æ•°æ®é‡å¾ˆå¤§ï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        confirm = input("   ç¡®è®¤ç»§ç»­ï¼Ÿ(yes/no) [no]: ").strip().lower()
        if confirm == "yes":
            cleaned_df = loader.process_multiple_years(available_years)
            if cleaned_df is not None:
                filename = f'weather_1901_2019_{time_suffix}_{detrend_suffix}_{discrete_suffix}.csv'
                loader.save_processed_data(cleaned_df, filename)
                loader.generate_summary_statistics(cleaned_df)
        else:
            print("   âœ— å·²å–æ¶ˆå¤„ç†å…¨éƒ¨æ•°æ®")

    print("\n" + "=" * 80)
    print("âœ… æ•°æ®å¤„ç†å®Œæˆï¼")
    print("=" * 80)


if __name__ == "__main__":
    main()
